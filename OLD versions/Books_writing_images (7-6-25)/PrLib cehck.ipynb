{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "188fbd12-5dc6-4467-b490-e3577c25c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_metadata(url):\n",
    "        #fetch metadata:\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "\n",
    "    html_text =requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    if url.split(\"https://\")[1][4:9]==\"prlib\": #prlib.ru metadata\n",
    "        title = soup.head.title.text.split(\"|\")[0]\n",
    "        full_title=title\n",
    "        if len(title)>210:\n",
    "            end_on_space=\" \".join(title[:210].split(\" \")[:-1])\n",
    "            title=end_on_space+\" ...\"\n",
    "        authors=soup.find(\"ul\",{\"class\":\"field field-name-field-book-author field-type-taxonomy-term-reference field-label-hidden\"})\n",
    "        author_=[]\n",
    "        try:\n",
    "            for author in authors.find_all(\"a\"):\n",
    "                author_.append(author.text)\n",
    "        except:\n",
    "            author_=\"\"\n",
    "            \n",
    "        description=\"\"\n",
    "        td_check=soup.find(\"div\",{\"class\":\"field field-name-field-book-bd field-type-text-long field-label-hidden\"}).find_all(\"td\")\n",
    "        if len(td_check)!=0: #check for zero td\n",
    "            for desc in td_check[1:][:-1]:\n",
    "                if desc.td is None and desc.get_text(strip=True)!=\"\":\n",
    "                    description+=desc.get_text(strip=True).replace(\"\\n\",\"\")+\"\\n\"\n",
    "        #collection+catalog form a subject:\n",
    "        #Catalogs\n",
    "        subjects=[]\n",
    "        catalogs=soup.find_all(class_=\"df-bbk\")\n",
    "        if len(catalogs)!=0:\n",
    "            for subject in catalogs[0].find_all(\"li\"):\n",
    "                subjects.append(subject.text.strip())\n",
    "        \n",
    "        #Collections: #ADD THEM to DESCRIPTION\n",
    "        subject_set=[]\n",
    "        collections=soup.find_all(class_=\"df-relations\")\n",
    "        if len(collections)!=0:\n",
    "            for subject in collections[0].find_all(\"li\"):\n",
    "                description+=subject.text+\"\\n\"\n",
    "                for element in subject.text.split(\" → \")[:-2]:\n",
    "                    subject_set.append(element)\n",
    "        subjects=subjects+list(dict.fromkeys(subject_set))\n",
    "        #language detection:\n",
    "        lang=detect(title)\n",
    "        dict_lang={\"de\":\"German\", \"en\":\"English\"}\n",
    "        if lang in list(dict_lang.keys()):\n",
    "            language=dict_lang[lang]\n",
    "        else:\n",
    "            language=\"Russian\"\n",
    "        \n",
    "        #add \"date\" after the data is scrapped\n",
    "        #search by date in .csv?\n",
    "        \n",
    "        dataset=\"PrLib_Dataset.csv\"\n",
    "        #adding date\n",
    "        date=\"\"\n",
    "        if os.path.exists(dataset):\n",
    "            with open(dataset, mode ='r',encoding=\"utf-8\")as file:\n",
    "              csvFile = csv.reader(file)\n",
    "              for lines in csvFile:\n",
    "                    if url==lines[2]:\n",
    "                        date=lines[0]\n",
    "    return {\n",
    "        \"collection\":[\"russian-online-libraries\"],\n",
    "        \"creator\" : author_,\n",
    "        \"language\" : language,\n",
    "        \"mediatype\" : \"texts\",\n",
    "        \"title\" : title,\n",
    "        \"Full_title\": full_title,\n",
    "        \"description\":   description,\n",
    "        \"subject\":subjects,\n",
    "        \"date\":date,\n",
    "        \"Source_url\": url\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc85936e-fa6b-44f6-aac8-eb3a869e6ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Петров, Михаил Назарович (историк; 1826-1887).\n",
      "Лекции по всемирной истории : [т. 1-5] / проф. М. Н. Петров. - Издание 2-е, исправленное и дополненное. -С.-Петербург : издал В. Березовский, 1904-1910. -5 т. ; 25 см. -Заглавие т. 5: Дополнение к лекциям по всемирной истории проф. М. Н. Петрова; перед заглавием автор: П. Н. Ардашев, проф. Ун-та св. Владимира. Указание на повторность издания отсутствует..\n",
      "I. Ардашев, Павел Николаевич (1865-1922). II. Березовский, Владимир Антонович (издатель; 1852-1917).1. Народ (коллекция). 2. Всеобщая история -- Учебные издания для высших учебных заведений.\n",
      "Источник электронной копии: РГБМесто хранения оригинала: РГБ\n",
      "Т. 4: История новых веков : (от Вестфальского мира до Конвента) / в обработке проф. В. П. Бузескула. -  1905. -X, 248 с.. -Библиография в подстрочных примечаниях..\n",
      "I. Бузескул, Владислав Петрович (1858-1931).1. Народ (коллекция). 2. Учебные издания по истории (коллекция). 3. Всеобщая история -- Учебные издания для высших учебных заведений. 4. Новая история -- 17 - 19 вв. -- Учебные издания для высших учебных заведений.\n",
      "Коллекции и информационные ресурсы Президентской библиотеки, вошедшие в единый учебник истории для 5–9 классов → Учебные издания по истории → Учебные издания по всемирной истории  → Издания для высших учебных заведений\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Петров, Михаил Назарович (историк; 1826-1887).\\nЛекции по всемирной истории : [т. 1-5] / проф. М. Н. Петров. - Издание 2-е, исправленное и дополненное. -С.-Петербург : издал В. Березовский, 1904-1910. -5 т. ; 25 см. -Заглавие т. 5: Дополнение к лекциям по всемирной истории проф. М. Н. Петрова; перед заглавием автор: П. Н. Ардашев, проф. Ун-та св. Владимира. Указание на повторность издания отсутствует..\\nI. Ардашев, Павел Николаевич (1865-1922). II. Березовский, Владимир Антонович (издатель; 1852-1917).1. Народ (коллекция). 2. Всеобщая история -- Учебные издания для высших учебных заведений.\\nИсточник электронной копии: РГБМесто хранения оригинала: РГБ\\nТ. 4: История новых веков : (от Вестфальского мира до Конвента) / в обработке проф. В. П. Бузескула. -  1905. -X, 248 с.. -Библиография в подстрочных примечаниях..\\nI. Бузескул, Владислав Петрович (1858-1931).1. Народ (коллекция). 2. Учебные издания по истории (коллекция). 3. Всеобщая история -- Учебные издания для высших учебных заведений. 4. Новая история -- 17 - 19 вв. -- Учебные издания для высших учебных заведений.\\nКоллекции и информационные ресурсы Президентской библиотеки, вошедшие в единый учебник истории для 5–9 классов → Учебные издания по истории → Учебные издания по всемирной истории  → Издания для высших учебных заведений\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1245caf3-0643-417e-a8bf-9ee7ac3f35c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'collection': ['russian-online-libraries'],\n",
       " 'creator': ['Бузескул Владислав Петрович',\n",
       "  'Ардашев Павел Николаевич',\n",
       "  'Березовский Владимир Антонович',\n",
       "  'Петров Михаил Назарович'],\n",
       " 'language': 'Russian',\n",
       " 'mediatype': 'texts',\n",
       " 'title': 'Лекции по всемирной истории. Т. 4. История новых веков ',\n",
       " 'Full_title': 'Лекции по всемирной истории. Т. 4. История новых веков ',\n",
       " 'description': 'Петров, Михаил Назарович (историк; 1826-1887).\\nЛекции по всемирной истории : [т. 1-5] / проф. М. Н. Петров. - Издание 2-е, исправленное и дополненное. -С.-Петербург : издал В. Березовский, 1904-1910. -5 т. ; 25 см. -Заглавие т. 5: Дополнение к лекциям по всемирной истории проф. М. Н. Петрова; перед заглавием автор: П. Н. Ардашев, проф. Ун-та св. Владимира. Указание на повторность издания отсутствует..\\nI. Ардашев, Павел Николаевич (1865-1922). II. Березовский, Владимир Антонович (издатель; 1852-1917).1. Народ (коллекция). 2. Всеобщая история -- Учебные издания для высших учебных заведений.\\nИсточник электронной копии: РГБМесто хранения оригинала: РГБ\\nТ. 4: История новых веков : (от Вестфальского мира до Конвента) / в обработке проф. В. П. Бузескула. -  1905. -X, 248 с.. -Библиография в подстрочных примечаниях..\\nI. Бузескул, Владислав Петрович (1858-1931).1. Народ (коллекция). 2. Учебные издания по истории (коллекция). 3. Всеобщая история -- Учебные издания для высших учебных заведений. 4. Новая история -- 17 - 19 вв. -- Учебные издания для высших учебных заведений.\\nКоллекции и информационные ресурсы Президентской библиотеки, вошедшие в единый учебник истории для 5–9 классов → Учебные издания по истории → Учебные издания по всемирной истории  → Издания для высших учебных заведений\\n',\n",
       " 'subject': ['Период 1789',\n",
       "  'Коллекции и информационные ресурсы Президентской библиотеки, вошедшие в единый учебник истории для 5–9 классов',\n",
       "  'Учебные издания по истории'],\n",
       " 'date': '1905',\n",
       " 'Source_url': 'https://www.prlib.ru/item/425841'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_metadata(\"https://www.prlib.ru/item/425841\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f671caca-9d5f-41b2-b14e-6558c761a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import codecs\n",
    "import datetime\n",
    "import errno\n",
    "import functools\n",
    "import hashlib\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "import requests\n",
    "import threading\n",
    "from bs4 import Tag\n",
    "from requests import Response\n",
    "from typing import Dict, Optional, Pattern, Union\n",
    "import json\n",
    "from random import randrange\n",
    "import csv\n",
    "import transliterate \n",
    "import internetarchive\n",
    "from internetarchive.session import ArchiveSession\n",
    "from internetarchive import search_items\n",
    "from langdetect import detect #language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc6ce464-719f-47ce-a8b9-b76ca382beb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'os_id': 'win',\n",
       " 'navigator_id': 'chrome',\n",
       " 'platform': 'Windows NT 5.1',\n",
       " 'oscpu': 'Windows NT 5.1',\n",
       " 'build_version': '83.0.4103.101',\n",
       " 'build_id': None,\n",
       " 'app_version': '5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.101 Safari/537.36',\n",
       " 'app_name': 'Netscape',\n",
       " 'app_code_name': 'Mozilla',\n",
       " 'product': 'Gecko',\n",
       " 'product_sub': '20030107',\n",
       " 'vendor': 'Google Inc.',\n",
       " 'vendor_sub': '',\n",
       " 'user_agent': 'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.101 Safari/537.36'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import user_agent\n",
    "user_agent.generate_navigator(os='win',device_type ='desktop',navigator='chrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd1828f-79a6-4138-bd4f-0790ba0ae4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import asyncio\n",
    "from aiohttp import ClientSession,TCPConnector, ClientTimeout\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "#import nest_asyncio #used for debugging\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import img2pdf\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from util import get_logger\n",
    "from util import md5_hex, to_float, cut_bom, perror, progress, ptext, safe_file_name, Browser, select_one_text_optional\n",
    "from util import select_one_text_required, select_one_attr_required, gwar_fix_json,mkdirs_for_regular_file\n",
    "from util import user_agents, number_of_images\n",
    "from user_agent import generate_user_agent\n",
    "\n",
    "import logging\n",
    "import threading\n",
    "import requests\n",
    "\n",
    "\n",
    "log = get_logger(__name__)\n",
    "BOOK_DIR = 'books'\n",
    "\n",
    "eshplDl_params = {\n",
    "    'quality': 8,\n",
    "    'ext': 'jpg'\n",
    "}\n",
    "\n",
    "prlDl_params = {\n",
    "    'ext': 'jpg' }\n",
    "\n",
    "headers_pr1 = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br, zstd\", \n",
    "    \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8,ru;q=0.7\",\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "    'If-Modified-Since': 'Tue, 20 Dec 2016 02:17:59 GMT',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Sec-Gpc':'1',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',\n",
    "    'dnt': '1',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"137\", \"Google Chrome\";v=\"137\", \"Not/A)Brand\";v=\"24\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-gpc': '1',\n",
    "    'Host':'content.prlib.ru',\n",
    "    'Origin':'https://content.prlib.ru'\n",
    "}\n",
    "\n",
    "bro: Browser\n",
    "bro=Browser(pause=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34460d4f-417d-4775-888c-f5709b150640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.111 Safari/537.36'}\n",
      "{'User-Agent': 'Mozilla/5.0 (Windows NT 5.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.111 Safari/537.36'}\n"
     ]
    }
   ],
   "source": [
    "from util import Browser\n",
    "bro=Browser(pause=1)\n",
    "url=\"https://www.prlib.ru/item/425843\"\n",
    "html_text = bro.get_text(url)\n",
    "soup = BeautifulSoup(html_text, 'html.parser')\n",
    "for script in soup.find_all('script'): #findAll deprecated\n",
    "    st = str(script)\n",
    "    if 'jQuery.extend' in st:\n",
    "        book_json = json.loads(st[st.find('{\"'): st.find(');')])\n",
    "        \n",
    "        try:\n",
    "            if \"item\" in url.split(\"prlib.ru/\")[1]:   #case for https://www.prlib.ru/item/*** \n",
    "                book = book_json['diva']['1']['options']\n",
    "            elif \"node\" in url.split(\"prlib.ru/\")[1]:     #case for https://www.prlib.ru/node/***\n",
    "                book = book_json['diva']['settings']\n",
    "        except:\n",
    "            log.exception(\"Error, NOTHING FOUND!\")\n",
    "            \n",
    "\n",
    "json_text = bro.get_text(book['objectData'])\n",
    "book_data = json.loads(json_text)\n",
    "pages = book_data['pgs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "056d8273-cd0d-4284-a4f3-a77411b7731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "page=pages[0]\n",
    "img_url = 'https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF={}/{}&JTL={},'.format(\n",
    "            book['imageDir'], page['f'], page['m']) #поменял здесь немного вид урл, так как по частям качаю\n",
    "        # брал урл отсюда: https://iipimage.sourceforge.io/documentation/protocol\n",
    "img_url+=\"{}\"\n",
    "\n",
    "headers_pr1.update({'Referer': url})\n",
    "width, height=number_of_images(page[\"d\"][len(page['d']) - 1]['w'],page[\"d\"][len(page['d']) - 1]['h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577f62c8-d295-45d7-bf9e-cad82a024f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d41757ac-1c5e-4187-9ca5-e75c7e3d0feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9665071964263916\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import nest_asyncio\n",
    "start=time.time()\n",
    "nest_asyncio.apply()\n",
    "try:\n",
    "    asyncio.run(async_images_download(img_url,width*height,headers_pr1))\n",
    "except Exception as Argument:\n",
    "    import logging\n",
    "    logging.exception(Argument)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29e173d5-091f-458d-8d53-8778a8e6aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_image_download(url: str, i, headers_pr1, sem,title):\n",
    "    \"\"\" Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)\n",
    "    по url скачиваю картинку и добавляю в binary в queue asyncio\n",
    "    \"\"\"\n",
    "    async with sem:\n",
    "        async with ClientSession(headers=headers_pr1,timeout=ClientTimeout(total=10),trust_env=True) as session: #,trust_env=True\n",
    "            async with session.get(url) as response:\n",
    "                with open(BOOK_DIR+\"\\\\\"+title+\"\\\\images\\\\\"+str(i)+\".jpg\",\"wb\") as file:\n",
    "                    file.write(await response.read())\n",
    "                \n",
    "                \n",
    "async def async_images_download(url,nums,headers_pr1,title):\n",
    "    \"\"\"Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)\n",
    "    call every tile image to download in async mode (in the end, add binary with the image number to results_prlDl\n",
    "    \"\"\"\n",
    "    \n",
    "    sem = asyncio.Semaphore(50)##https://stackoverflow.com/questions/63347818/aiohttp-client-exceptions-clientconnectorerror-cannot-connect-to-host-stackover\n",
    "    #queue = asyncio.Queue()\n",
    "    async with asyncio.TaskGroup() as group: #https://blog.csdn.net/y662225dd/article/details/135273140\n",
    "        for i in nums:\n",
    "            headers_pr1.update({'User-Agent': generate_user_agent(os=('win')) })\n",
    "            group.create_task(fetch_image_download(url.format(i), i,headers_pr1,sem,title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29f8d22c-e06c-4616-9aef-8003b04d2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Postprocess(images_folder,width, height,image_path):\n",
    "    \"\"\"\n",
    "     Прохожу через бинарные данные в results_prlDl, ставлю их на правильные места в картинке исходной и вывожу все в файл, напртмер 0001.jpg\n",
    "    \"\"\"\n",
    "    Total_Image=[i for i in range(width*height)]\n",
    "    #iterate through each file and add them:\n",
    "    for item in range(width*height):\n",
    "        #read from file:\n",
    "        Total_Image[item]=CV2_Russian(os.path.join(images_folder, str(item)+\".jpg\")) # название папки на Русском в названии мешало прочитать cv2 файл (это окалаось известный баг cv2)\n",
    "    #delete images folder:\n",
    "    shutil.rmtree(images_folder)\n",
    "    \n",
    "    regroup=[]\n",
    "    for h in range(height):\n",
    "        regroup.append(Total_Image[h*width:(h+1)*width])\n",
    "    try:\n",
    "        im_h=cv2.vconcat([cv2.hconcat(item) for item in regroup])\n",
    "\n",
    "    #cv2.imwrite(image_path, im_h) (doesn't work with Russian)\n",
    "        result, data = cv2.imencode('.jpg', im_h)\n",
    "    except:\n",
    "        \n",
    "        return False\n",
    "    fh = open(image_path, 'wb')\n",
    "    fh.write(data)\n",
    "    fh.close()\n",
    "    return True\n",
    "def CV2_Russian(name):\n",
    "    \"\"\"\n",
    "    Чтение картинки с русским названием в пути в cv2\n",
    "    #https://answers.opencv.org/question/205345/imread-and-russian-language-path-to-img/\n",
    "    \"\"\"\n",
    "    f = open(name, \"rb\")\n",
    "    chunk = f.read()\n",
    "    chunk_arr = np.frombuffer(chunk, dtype=np.uint8)\n",
    "    img = cv2.imdecode(chunk_arr, cv2.IMREAD_COLOR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d351cd8c-90bc-4af5-b964-9d44694e18e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=headers_pr1.update({\"Host\":\"www.prlib.ru\",\"Origin\": \"https://www.prlib.ru\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eddaa749-894a-4fc4-9830-c01769d0b0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8,ru;q=0.7', 'Cache-Control': 'max-age=0', 'Connection': 'keep-alive', 'If-Modified-Since': 'Tue, 20 Dec 2016 02:17:59 GMT', 'Sec-Fetch-Dest': 'document', 'Sec-Fetch-Mode': 'navigate', 'Sec-Fetch-Site': 'none', 'Sec-Fetch-User': '?1', 'Sec-Gpc': '1', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36', 'dnt': '1', 'sec-ch-ua': '\"Chromium\";v=\"137\", \"Google Chrome\";v=\"137\", \"Not/A)Brand\";v=\"24\"', 'sec-ch-ua-mobile': '?0', 'sec-ch-ua-platform': '\"Windows\"', 'sec-gpc': '1', 'Host': 'www.prlib.ru', 'Origin': 'https://www.prlib.ru'}\n"
     ]
    }
   ],
   "source": [
    "print(headers_pr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36176121-c6ba-4088-93c1-58c6710bfb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=\"books\\\\Каталог картин, принадлежавших действительному тайному советнику Ф.И. Прянишникову и помещтябрь 1867 года\"\n",
    "shutil.rmtree(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1276d3e4-7aff-4126-a747-8f8bd3a610ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF=/var/data/scans/public/7E028BF3-4CBD-429A-9FBB-48364FC9032B/0/10000066_doc1.tiff&JTL=3,{}\n",
      "https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF=/var/data/scans/public/7E028BF3-4CBD-429A-9FBB-48364FC9032B/0/10000067_doc1.tiff&JTL=3,{}\n",
      "7.455045461654663\n"
     ]
    }
   ],
   "source": [
    "STOP_break=False\n",
    "import nest_asyncio\n",
    "import shutil\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "prlDl(\"https://www.prlib.ru/item/710798\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07ba6443-eaf3-4a0b-abb3-9aeef0152459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prlDl(url):\n",
    "    \"\"\"\n",
    "    Президентская библиотека имени Б.Н. Ельцина\n",
    "    Формат - серия изображений\n",
    "    Пример урла книги (HTML) - https://www.prlib.ru/item/420931\n",
    "    \"\"\"\n",
    "    ext = prlDl_params['ext']\n",
    "    global headers_pr1\n",
    "    headers_pr2=headers_pr1 \n",
    "    headers_pr2.update({\"Host\":\"www.prlib.ru\",\"Origin\": \"https://www.prlib.ru\"})\n",
    "    html_text = requests.get(url, headers=headers_pr2).text\n",
    "    \n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    title = soup.head.title.text.split(\"|\")[0]\n",
    "    title = safe_file_name(title)\n",
    "    #get the number of characters in the current path\n",
    "    num_of_characters=150-len(os.path.abspath(os.getcwd()))\n",
    "    if len(title)>165:\n",
    "        title=title[:num_of_characters] + title[-15:]#to have the volume part in the name\n",
    "    \n",
    "    log.info(f'Каталог для загрузки: {title}')\n",
    "    \n",
    "    for script in soup.find_all('script'): #findAll deprecated\n",
    "        st = str(script)\n",
    "        if 'jQuery.extend' in st:\n",
    "            book_json = json.loads(st[st.find('{\"'): st.find(');')])\n",
    "            \n",
    "            try:\n",
    "                if \"item\" in url.split(\"prlib.ru/\")[1]:   #case for https://www.prlib.ru/item/*** \n",
    "                    book = book_json['diva']['1']['options']\n",
    "                elif \"node\" in url.split(\"prlib.ru/\")[1]:     #case for https://www.prlib.ru/node/***\n",
    "                    book = book_json['diva']['settings']\n",
    "            except:\n",
    "                log.exception(\"Error, NOTHING FOUND!\")\n",
    "                return\n",
    "    json_text = bro.get_text(book['objectData'])\n",
    "   \n",
    "    book_data = json.loads(json_text)\n",
    "    pages = book_data['pgs']\n",
    "    num_of_pages_down=1 #for the time prediction\n",
    "    start=datetime.datetime.now()#for the time prediction\n",
    "    global STOP_break\n",
    "    counter=0 #check the pages\n",
    "    while counter<2:\n",
    "        idx=counter\n",
    "        page=pages[counter]\n",
    "        #force downloading every page, if error REPEAT\n",
    "        if STOP_break:\n",
    "            break\n",
    "        img_url = 'https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF={}/{}&JTL={},'.format(\n",
    "            book['imageDir'], page['f'], page['m']) #поменял здесь немного вид урл, так как по частям качаю\n",
    "        # брал урл отсюда: https://iipimage.sourceforge.io/documentation/protocol\n",
    "        img_url+=\"{}\"\n",
    "        width, height=number_of_images(page[\"d\"][len(page['d']) - 1]['w'],page[\"d\"][len(page['d']) - 1]['h'])\n",
    "        \n",
    "        image_short = '%05d.%s' % (idx+1, ext)\n",
    "        image_path = os.path.join(BOOK_DIR, title, image_short)\n",
    "        print(img_url)\n",
    "        # заменяю все фичи ручками (например тут skip_if_exists), которые были ранее доступны через функции \n",
    "        #(т.к. метод у меня скачивания немного другой)\n",
    "        if os.path.exists(image_path) and os.stat(image_path).st_size > 0:\n",
    "            log.info(f'Пропускаю скачанный файл: {image_path}')\n",
    "            counter+=1\n",
    "           \n",
    "            #progress(f'  Прогресс: {idx + 1} из {len(pages)} стр. ')\n",
    "        else: \n",
    "          \n",
    "            #mkdirs_for_regular_file(image_path)\n",
    "            \n",
    "            images_folder=os.path.join(BOOK_DIR, title,\"images\")\n",
    "            nums=range(width*height)\n",
    "            try:\n",
    "                os.makedirs(images_folder)\n",
    "            except FileExistsError:\n",
    "                pass\n",
    "            nest_asyncio.apply() # нужен только чтобы async работал нормально в Jupyter ( https://pypi.org/project/nest-asyncio/)\n",
    "            # получить все данные с картиники:\n",
    "\n",
    "            headers_pr1.update({'Referer': url})\n",
    "            \n",
    "            flag=True #для проверки на хороший requests\n",
    "            #check for all images already downloaded?\n",
    "            \n",
    "            while flag: #just keep quering the connection\n",
    "                #check what files were already downloaded:\n",
    "                good_nums=[]\n",
    "                for num in nums:\n",
    "                    if not (os.path.isfile(os.path.join(images_folder,str(num)+\".jpg\")) and os.path.getsize(os.path.join(images_folder,str(num)+\".jpg\"))!=0):\n",
    "                        good_nums.append(num)\n",
    "                nums=good_nums\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    asyncio.run(async_images_download(img_url,nums,headers_pr1,title)) #Downgrade to 3.6.2  #Using Python 3.8 https://blog.csdn.net/y662225dd/article/details/135273140\n",
    "                    #loop = asyncio.get_event_loop() #for old version of aiohttp: 3.6.2\n",
    "                    #loop.run_until_complete(async_images(img_url,width*height,headers))\n",
    "                except Exception as Argument:  #Error coding\n",
    "                    time.sleep(2.)\n",
    "\n",
    "                    log.exception(\"Error occurred in ASYNCIO\") \n",
    "                else:\n",
    "                    check=False\n",
    "                    time.sleep(1.0)\n",
    "                    lst=os.listdir(images_folder)\n",
    "                    if len(lst)==width*height:\n",
    "                        check=True\n",
    "                    #check for each file to be non empty:\n",
    "                    for file in lst:\n",
    "                        if os.path.getsize(os.path.join(images_folder, file))==0:\n",
    "                            os.remove(os.path.join(images_folder, file))\n",
    "                            check*=False\n",
    "                        #else: check stays True\n",
    "                    if check: #stop repeating                        \n",
    "                        flag=False\n",
    "            \n",
    "            # просессить все данные и в конце вывести картинку\n",
    "            if Postprocess(images_folder,width,height, image_path):\n",
    "              counter+=1\n",
    "              num_of_pages_down+=1\n",
    "            \n",
    "           \n",
    "    return title, ext"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
