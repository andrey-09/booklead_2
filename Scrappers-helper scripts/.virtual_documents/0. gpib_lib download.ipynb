


import csv


datafile="..\\databases\\FULL_BOOKS_GPIB_DONE.csv"
with open(datafile,"r", encoding='utf-8-sig') as csvfile: #read the place, where to put value
    f = csv.reader(csvfile,delimiter=";")
    data=list(f)


title=[]
for i in data[1:]:
    title.append(i[1])



# Create an empty dictionary to store the counts
freqs = {}

# Loop through the list
for c in words:
    # If the item is already in dictionary, increase its count
    if c in freqs:
        freqs[c] += 1
    # If the item is not in dictionary, add it with a count of 1
    else:
        freqs[c] = 1


words=[]
count=0
for sen in title:
    words=words+sen.split(" ")
    if count%100==0:
        print(count)
    count+=1


from internetarchive import get_session
c = {'s3': {'access': 'qJaX9KKXhXkzoN5o', 'secret': 'mmI4XUkxM9O8gZ15'}}
s = get_session(config=c)
query='uploader:"pavelserebrjanyi@gmail.com" AND mediatype:texts AND source_url:*elib.shpl.ru*'
items=s.search_items(query, fields=["source_url"])
source_urls=[]
for item in items:
    source_urls.append(item["source_url"])





import csv
import re
url="http://elib.shpl.ru/ru/nodes/98765-16-marta-1912-g-89007-188-locale-nil-1912"
database="FULL_BOOKS_GPIB.csv"


with open(database,"r",encoding="utf-8-sig") as base:
    csvFile=csv.reader(base,delimiter=";")
    for row in csvFile:
        if row[2]==url:
            #get all the data:
            if row[0]!="":
                date=row[0]
            else:
                date=""
            if row[4]!="":
                description=row[4].replace('\xa0', ' ')
            else:
                description=row[1].replace('\xa0', ' ')
            if row[7]!="":
                author_=row[7].replace('\xa0', ' ')
            else:
                author_=""
            if row[1]!="":
                title=row[1][:230].replace('\xa0', ' ')
                full_title=row[1].replace('\xa0', ' ')
            else:
                title=row[8][:230].replace('\xa0', ' ')
                full_title=row[8].replace('\xa0', ' ')
            if row[6]!="":
                res=re.split(r'\(\d+\)',row[6])
                subjects=[i.replace('\xa0', ' ').strip() for i in res]
            else:
                subjects=""
            #language detection:
            try:
                lang=detect(title)
            except:
                lang=''
            dict_lang={"de":"German", "en":"English"}
            if lang in list(dict_lang.keys()):
                language=dict_lang[lang]
            else:
                language="Russian"
print({"creator" : author_,
        "language" : language,
        "mediatype" : "texts",
        "title" : title,
        "Full_title": full_title,
        "description":   description,
        "subject":subjects,
        "date":date,
        "Source_url": url})


with open(database,"r",encoding="utf-8-sig") as base:
    csvFile=csv.reader(base,delimiter=";")
    url="http://elib.shpl.ru/ru/nodes/49403-1911-1912-gg-kn-1-1913"
    for row in csvFile:
        print([(k,v) for k,v in enumerate(row)])
        break


import csv



url="http://elib.shpl.ru/ru/nodes/49403-1911-1912-gg-kn-1-1913"
database="FULL_BOOKS_GPIB.csv"
with open(database,"r",encoding="utf-8-sig") as base:
    csvFile=csv.reader(base,delimiter=";")
    for row in csvFile:
        if row[2]==url:
            pages=row[10].split(", ")


import urllib
urllib.parse.urlsplit(url).netloc














#urls_origin=['http://elib.shpl.ru/ru/indexes/values/111532?per_page=200&page={}'.format(i) for i in range(34,146)]
with open("GPIB_Agitation.txt","r") as file:
    urls_origin=file.read().splitlines()


from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

driver = webdriver.Chrome()
urls=[]
titles=[]


from selenium.webdriver.support import expected_conditions as EC

count=0
for url_origin in urls_origin:
    #resp=requests.get(url_origin, headers=headers_pr)
    #html=resp.text
    driver.get(url_origin)
    wait = WebDriverWait(driver, 60)
    element = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'nodes-view')))
    
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    all_items=soup.find_all(class_="nodes-grid")[0].find_all("li")
    for item in all_items:
        try:
            url='http://elib.shpl.ru'+item.a["href"]
            title=item.a.find("img")["alt"]
        except:
            continue
        urls.append(url)
        titles.append(title)
    print(count)
    count+=1


data=[[titles[i],urls[i]] for i in range(len(urls))]
data.insert(0,["title","url"])


import csv
with open('Books_GPIB.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerows(data)

"""
with open('Books_GPIB.csv', 'r', encoding='utf-8') as f:
    writer = csv.reader(f)
    data=list(writer)

data.insert(0,["title","url"])



"""






import asyncio

async def download_image(url,session,idx,sem1):
    async with sem1:
        async with session.get(url) as response:
            if response.ok:  
                with open("images\\"+str(idx)+".jpg","wb") as file1:              
                    file1.write(await response.read()) 
async def create_download(pages):
    sem1 = asyncio.Semaphore(10)
    async with ClientSession(timeout=ClientTimeout(total=30),headers=hedaders_1) as session:
        tasks = [asyncio.create_task(download_image(f'http://elib.shpl.ru//pages/{page["id"]}/zooms/8',session,idx,sem1)) for idx, page in enumerate(pages)]
        results = await asyncio.gather(*tasks)


import time
start=time.time()
eshplDl('http://elib.shpl.ru/ru/nodes/95737-i-vedet-etu-armiyu-stalin-locale-nil-m-1934')
print(time.time()-start)


import requests
import nest_asyncio
def eshplDl(url):
    ext = '.jpg'
    quality = 8
    domain = urllib.parse.urlsplit(url).netloc
    html_text = requests.get(url,headers=hedaders_1).text
    
    #print(html_text)
    soup = BeautifulSoup(html_text, 'html.parser')
    #title = select_one_text_optional(soup, 'title') or md5_hex(url)
    #title = safe_file_name(title)
    for script in soup.find_all('script'):   
        st = str(script)      
        if 'initDocview' in st:
            book_json = json.loads(st[st.find('{"'): st.find(')')])
    pages = book_json['pages']
    nest_asyncio.apply()
    asyncio.run(create_download(pages))
   
    return True



import shutil
hedaders_1={
'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
'Accept-Encoding': 'gzip, deflate',
'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8,ru;q=0.7',
'Cache-Control': 'max-age=0',
'Connection': 'keep-alive',
'Cookie': 'ahoy_visitor=e89e88c5-5cfe-4cbc-99d7-db554c5b590f; ahoy_visit=ea23e526-abc4-441e-82bd-b12b87ea51bd; _platform_session=u%2Fw%2B02eobjNyxpdUXwmMmPLpVAfdnxnFFY2X9CbZFBv5%2FHGveIt3kQBSWHk05oqxl%2B7%2BwybGXDhBxJXnvHtapmTeex%2Fo3lMDvJFhkVFmnzg4jXXlH9DF9TzkatrN2b7D0EQ%2BM4Y82mhm6pq8TZZhHk08d6fFyF4d6aEN0YSSnlMwLsfVVBWOxU5B7lq9e0aMd7SljZnFHEIRSI2oWS3H6yJ5RMWCrJQIWLhFvYduN0NHV524iwU8FMqREOi9BYyaAucrFQTEp4r0qtJ5yOapyvCHQPZ49I9jPgE19NYVciIm915EGHq4eWwEL4NOx6hEhdWEs1020utyzCGW1zWxsQWSMlyG5%2BU632erUjH1TtngXUBd9JwSUAT1UNuBkRU6BkAYd6QRqo8bcxqavOR22W5mZDCvQzHySyqbkqmj2EFPZ2OTAkPcUicptY0npNSDejtR9thyCeD1JKhynjH6r3Kx1kCX7iqUsrcaPpQGDBnahDVCIXyfCYiMvFUFz9jd1AI5Bc4HvF0PQb7zEEhxWjPoMrlG7rFSiZUcUoNXyVYWbV4lucdgb%2FODYAvwsfFHy0s48Ra%2Be334VD%2FaQD%2FjbJkBhzVv%2FWaakoITWxXZnPmhHVkf5RClAk5XKOEAV9qLQk%2BAgwE%2BMTYmMnp4rwiPIGjtUxsVxRRqU%2Bol7ILFOlLJvyHO2DFqLU90EdnIo6O3FkK4s8EzmjaZw1Qk6ScHkAbjO69Rw5VzKiJuYIxpz9r5UHknqrPu2CU52HdUWQRnGK2k9%2FyJWwrupD4%2BWSEcCwHognAhLZPlYw%3D%3D--UPGzuhyp2iLY48RW--IjLVYayGjnr7AOtz9T6bug%3D%3D' ,
'Host': 'elib.shpl.ru',
'If-None-Match': 'W/"64989674aff36611149027327f12df67"',
'Upgrade-Insecure-Requests': '1',
'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',
'dnt': '1',
'sec-gpc': '1'
}





import argparse
import json
import os
import re
import urllib.parse
import asyncio
from aiohttp import ClientSession,TCPConnector, ClientTimeout
import datetime
import time
import numpy as np

import cv2
import random
import img2pdf
from bs4 import BeautifulSoup
import sys


class Browser:

    def __init__(self, pause: float):
        self.pause = pause

    
    def get_text(self, url: str, headers: Dict = None, content_type: str = None):
        headers = self._prepare_headers(headers)
        log.info(f'Запрашиваю GET {url}')
        log.info(f'Заголовки: {headers}')
        response = requests.get(url, headers=headers)
        log.info(f'Ответ: {response.status_code} {response.reason}')
        log.info(f'Заголовки: {response.headers}')
        self._validate_response(response, url, content_type)
        return response.text

 
    def post_text(self, url: str, headers: Dict = None, data: Dict = None, content_type: str = None):
        headers = self._prepare_headers(headers)
        log.info(f'Запрашиваю POST {url}')
        log.info(f'Заголовки: {headers}')
        response = requests.post(url, headers=headers, data=json.dumps(data))
        log.info(f'Ответ: {response.status_code} {response.reason}')
        log.info(f'Заголовки: {response.headers}')
        self._validate_response(response, url, content_type)
        return response.text


    def download(self, url: str,
                 fpath: str,
                 headers: Dict = None,
                 content_type: Union[str, Pattern] = None,
                 skip_if_file_exists=False):
        global last_time_connected
        progress(f' - Скачиваю {url}')
        if skip_if_file_exists and os.path.exists(fpath) and os.stat(fpath).st_size > 0:
            log.info(f'Пропускаю скачанный файл: {fpath}')
            last_time_connected = None
            return
        headers = self._prepare_headers(headers)
        log.info(f'Запрашиваю GET {url}')
        log.info(f'Заголовки: {headers}')
        response = requests.get(url, stream=True, headers=headers)
        log.info(f'Ответ: {response.status_code} {response.reason}')
        log.info(f'Заголовки: {response.headers}')
        self._validate_response(response, url, content_type)
        mkdirs_for_regular_file(fpath)
        with open(fpath, 'wb') as fd:
            shutil.copyfileobj(response.raw, fd)
        length = os.stat(fpath).st_size
        ptext(f' - Сохранено в файл {fpath} ({length} байт)')

    def _prepare_headers(self, additional_headers: Dict):
        headers = additional_headers if additional_headers else {}
        headers.update({'User-Agent': random.choice(user_agents)})
        return headers

    def _validate_response(self, response: Response, url, expected_ct: Union[str, Pattern]):
        if not response.ok:
            raise Exception(f'Не удалось скачать файл {url} - {response.status_code} {response.reason}')
        if expected_ct:
            actual_ct: str = response.headers.get('content-type')
            if actual_ct:
                if isinstance(expected_ct, Pattern):
                    if not expected_ct.match(actual_ct):
                        perror(f'Некорректный content-type {actual_ct} по адресу {url}')
                else:
                    if actual_ct != expected_ct:
                        perror(f'Некорректный content-type {actual_ct} по адресу {url}')
