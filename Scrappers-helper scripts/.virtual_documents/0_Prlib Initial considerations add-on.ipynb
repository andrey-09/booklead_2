


#CHECK On the DATA in ARCHIVE.ORG
from internetarchive import get_session
day='5'
c = {'s3': {'access': 'qJaX9KKXhXkzoN5o', 'secret': 'mmI4XUkxM9O8gZ15'}}
s = get_session(config=c)
query='uploader:"pavelserebrjanyi@gmail.com" AND mediatype:texts AND addeddate:2025-06-0'+day
items=s.search_items(query, fields=["source_url"])
source_urls=[]
for item in items:
    source_urls.append(item["source_url"])
    print(item["source_url"])
with open("5july_urls.txt","r") as file:
    urls=file.read().splitlines()
for url in urls:
    if url in source_urls:
        print(url)



import csv
day='8'
#update Excel:
# set everything to 0:
from internetarchive import get_session
c = {'s3': {'access': 'qJaX9KKXhXkzoN5o', 'secret': 'mmI4XUkxM9O8gZ15'}}
s = get_session(config=c)
query='uploader:"pavelserebrjanyi@gmail.com" AND mediatype:texts AND addeddate:2025-06-0'+day
items=s.search_items(query, fields=["source_url"])
source_urls=[]
for item in items:
    source_urls.append(item["source_url"])
with open(day+"julysource_urls.txt","w") as file:
    file.write("\n".join(source_urls))


import csv
import time
start=time.time()
#update Excel:
# set everything to 0:
from internetarchive import get_session
c = {'s3': {'access': 'qJaX9KKXhXkzoN5o', 'secret': 'mmI4XUkxM9O8gZ15'}}
s = get_session(config=c)
query='uploader:"pavelserebrjanyi@gmail.com" AND mediatype:texts'
items=s.search_items(query, fields=["source_url"])
source_urls=[]
for item in items:
    source_urls.append(item["source_url"])

datafile="..\\databases\\PrLib_Dataset_DONE overview (0-2000).csv"
with open(datafile,"r", encoding='windows-1251') as csvfile: #read the place, where to put value
    f = csv.reader(csvfile)
    data=list(f)
with open(datafile[:-4]+"_BACKUP.csv", 'w', newline='', encoding='windows-1251') as file: #put the value
    writer = csv.writer(file)
    writer.writerows(data)
for url in source_urls:
    url_column=[i[2] for i in data]
    try:
        ind=url_column.index(url)
        data[ind][4]=1
    except:
        continue
                        
with open(datafile, 'w', newline='', encoding='windows-1251') as file: #put the value
    writer = csv.writer(file)
    writer.writerows(data)

print("Time taken: ", time.time()-start)





from internetarchive import get_session
c = {'s3': {'access': 'qJaX9KKXhXkzoN5o', 'secret': 'mmI4XUkxM9O8gZ15'}}
s = get_session(config=c)


#GET ALL THE URLS
query='uploader:"pavelserebrjanyi@gmail.com" AND mediatype:texts'
items=s.search_items(query, fields=["source_url"])
source_urls=[]
for item in items:
    source_urls.append(item["source_url"])
#with open("source_urls.txt","w") as file:
#    file.write("\n".join(source_urls))


with open("source_urls.txt","r") as file:
    source_urls=file.read().splitlines()
print(len(source_urls))


"https://www.prlib.ru/item/1286872" not in source_urls


import os  # import os module
import internetarchive
#All items

directory = 'Books_history_Colab\\1-20 6-6-25'  # set directory path


for entry in os.scandir(directory):  
    if entry.is_file() and ("books_history_4" in entry.name):  # check if it's a file
        print(entry.path)
        good_urls=[]
        with open(entry.path, "r+") as file:
            urls=file.read().splitlines()
            #check each url, just search by source_url:
            for url in urls:
                if url not in source_urls:
                    good_urls.append(url)
        with open(entry.path, "w") as file:
            file.write("\n".join(good_urls))











session = {'access': 'qJaX9KKXhXkzoN5o', 'secret': 'mmI4XUkxM9O8gZ15'}


from internetarchive import get_session
c = {'s3': {'access': 'qJaX9KKXhXkzoN5o', 'secret': 'mmI4XUkxM9O8gZ15'}}
s = get_session(config=c)


import internetarchive
#All items
items=s.search_items('uploader:"pavelserebrjanyi@gmail.com" AND mediatype:texts AND NOT source_url:*prlib.ru* AND addeddate:[2025-05-12 TO 2025-06-05]', fields=["identifier","title","year"])





"1729" in "1729"


for item in items:
    print(item["year"])
    break


# Add Full_title + Add Source_url
import csv
from bs4 import BeautifulSoup
import requests
from internetarchive import modify_metadata

counter=0
for item in items:
    urls=[]
    #check for title in Dataset:
    with open("PrLib_Dataset.csv", mode ='r',encoding="utf-8")as file:
        csvFile = csv.reader(file)
        for lines in csvFile:
            if len(item["title"])>210:
                cond=item["title"] in lines[1]
            else:
                cond=item["title"] == lines[1]
            if cond and str(item["year"]) in lines[0]:
                urls.append(lines[2])
    if len(urls)==0:
        print("NOT FOUND ", item["identifier"])
    elif len(urls)>1:
        print("DUPLICATES ", item["identifier"])
    else:
        url=urls[0]
        html_text =requests.get(url).text
        soup = BeautifulSoup(html_text, 'html.parser')
        #modify metadata:
        title = soup.head.title.text.split("|")[0]
        modify_metadata(item["identifier"], metadata={"Source_url": url, "Full_title": title}, access_key =session["access"], secret_key =session["secret"])



urls











import internetarchive
items=s.search_items('uploader:"pavelserebrjanyi@gmail.com" AND mediatype:texts AND description:*prlib.ru* AND addeddate:[* TO 2025-06-0]', fields=["description", "identifier","title"])



from internetarchive import modify_metadata
from langdetect import detect
dict_lang={"de":"German", "en":"English"}
for item in items:
    lang=detect(item["title"])
    if lang in ["en","de"]:
        iden=item["identifier"]
        print(item["identifier"])
        print(lang)
        print()
        modify_metadata(iden, metadata={'language': dict_lang[lang]}, access_key =session["access"], secret_key =session["secret"])





import requests
from bs4 import BeautifulSoup
html_text = requests.get("https://www.prlib.ru/item/353936").text
soup = BeautifulSoup(html_text, 'html.parser')


description=""
td_check=soup.find("div",{"class":"field field-name-field-book-bd field-type-text-long field-label-hidden"}).find_all("td")
if len(td_check)!=0:
    for desc in td_check[2:][:-1]:
        if desc.td is None:
            description+=desc.get_text(strip=True).replace("\n","")+"\n"


print(description)


len(td_check)


td_check[3:][:-1]


for desc in soup.find("div",{"class":"field field-name-field-book-bd field-type-text-long field-label-hidden"}).find_all("td")[2:]:
    print(desc.get_text(strip=True))
    print(1)


import os

root="books\\Житие и славныя дела государя императора Петра Великаго самодержца всероссийскаго. [Ч. 2]"
for dir, subdirs, files in os.walk(root):
    for f in files:
        f_new=f[-9:]
        os.rename(os.path.join(root, f), os.path.join(root, f_new))


import os
os.path.abspath(os.getcwd())[-5:]





import csv
from internetarchive.session import ArchiveSession
from internetarchive import search_items
import datetime

#find all titles, by which i'll search later on

def CheckArchiveForWrites(urls):
    all_titles=[]
    datafile="Prlib_1600-1800.csv"
    with open(datafile, mode ='r',encoding="utf-8")as file:
        csvFile = csv.reader(file)
        for lines in csvFile:
            for url in urls:
                if url==lines[2] and lines[3]!="1": 
                    all_titles.append(lines[1]) #write all titles to search for


    s = ArchiveSession()
    today=datetime.datetime.today().strftime('%Y-%m-%d')
    search = s.search_items('uploader:"pavelserebrjanyi@gmail.com"', fields=["title"])
    for title in all_titles:
        for result in search:
            if title==result["title"]: #so, there is a common thing; change the csv value:
                
                with open(datafile,"r", encoding='utf-8') as csvfile: #read the place, where to put value
                    f = csv.reader(csvfile)
                    data=list(f)
                    title_column=[i[1] for i in data]
                    ind=title_column.index(title)
                    data[ind][3]=1
                    print(data[ind])
                #with open(datafile, 'w', newline='', encoding='utf-8') as file: #put the value
                #    writer = csv.writer(file)
                #    writer.writerows(data)

CheckArchiveForWrites(["https://www.prlib.ru/item/713798","https://www.prlib.ru/item/1599870"])





import csv
url="https://www.prlib.ru/item/408203"
with open('PrLib_Dataset.csv', mode ='r',encoding="utf-8")as file:
  csvFile = csv.reader(file)
  for lines in csvFile:
        if url==lines[2]:
            date=lines[0]
            print(lines)








from bs4 import BeautifulSoup
soup = BeautifulSoup(website, 'html.parser')


#Collections: #ADD THEM to DESCRIPTION
descr=""
subject_set=[]
collections=soup.find_all(class_="df-relations")
if len(collections)!=0:
    for subject in collections[0].find_all("li"):
        descr+=subject.text+"\n"
        for element in subject.text.split(" → ")[:-2]:
            subject_set.append(element)
list(dict.fromkeys(subject_set))+list(dict.fromkeys(subject_set))


print(descr)


#Catalogs
catalogs=soup.find_all(class_="df-bbk")
if len(catalogs)!=0:
    for subject in catalogs[0].find_all("li"):
        subjects.append(subject.text.strip())


subjects


soup.find_element(By.CLASS_NAME,"df-bbk")














user_agents = [
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36',
    'Mozilla/5.0 (Windows NT 5.1; rv:7.0.1) Gecko/20100101 Firefox/7.0.1',
    'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0'
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36'
]


### GPIB
import urllib
import shutil
from bs4 import BeautifulSoup
import requests
import os, random, json, re
eshplDl_params = {
    'quality': 8,
    'ext': 'jpg'
}

url="http://shpl.dlibrary.org/ru/nodes/34400-akulinin-s-istoriya-110-str-imeni-tov-baginskogo-polka-37-novocherkasskoy-str-divizii-im-estonskogo-proletariata-novozybkov-1928"
domain = urllib.parse.urlsplit(url).netloc
ext = eshplDl_params['ext']
quality = eshplDl_params['quality']
headers_eph1=({'User-Agent': random.choice(user_agents)})
html_text = requests.get(url, headers=headers_eph1).text
soup = BeautifulSoup(html_text, 'html.parser')
title = select_one_text_optional(soup, 'title') or md5_hex(url)
title = safe_file_name(title)

for script in soup.find_all('script'):
    st = str(script)
    if 'initDocview' in st:
        book_json = json.loads(st[st.find('{"'): st.find(')')])

pages = book_json['pages']

for idx, page in enumerate(pages):
    img_url = f'http://{domain}/pages/{page["id"]}/zooms/{quality}'
    image_short = '%05d.%s' % (idx+1, ext)
    image_path = os.path.join("books_test", title, image_short)
    mkdirs_for_regular_file(image_path)
    headers_eph1.update({'Referer': url})
    
     #check for BEFORE
   # if os.path.exists(image_path) and os.stat(image_path).st_size > 0:
   #             log.info(f'Пропускаю скачанный файл: {fpath}')
                #retur


headers_eph1= {
    "Accept": 
      "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
    ,
    "Accept-Encoding": 
      "gzip, deflate, br, zstd"
    ,
    "Accept-Language": 
      "en-US,en;q=0.9"
    ,
    "Connection": 
      "keep-alive"
    ,
    "Dnt": 
      "1"
    ,
    "Host": 
      "httpbin.io"
    ,
    "Sec-Ch-Ua": 
      "\"Chromium\";v=\"136\", \"Google Chrome\";v=\"136\", \"Not.A/Brand\";v=\"99\""
    ,
    "Sec-Ch-Ua-Mobile": 
      "?0"
    ,
    "Sec-Ch-Ua-Platform": 
      "\"Windows\""
    ,
    "Sec-Fetch-Dest": 
      "document"
    ,
    "Sec-Fetch-Mode": 
      "navigate"
    ,
    "Sec-Fetch-Site": 
      "cross-site"
    ,
    "Sec-Fetch-User": 
      "?1"
    ,
    "Sec-Gpc": 
      "1"
    ,
    "Upgrade-Insecure-Requests": 
      "1"
    ,
    "User-Agent": 
      "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36"
    
  }



lst = os.listdir(os.path.join("books", "ГПИБ Вып. 1 Жилищное строительство в городских поселениях РСФСР, Украинской ССР и Белорусской ССР. - 1927"))


len(lst)


url="http://shpl.dlibrary.org/pages/"
urls=[url+str(param)+"/zooms/8" for param in range(2750455, 2759475) ]
headers_eph1.update({'Referer': "http://shpl.dlibrary.org/ru/nodes/34400-akulinin-s-istoriya-110-str-imeni-tov-baginskogo-polka-37-novocherkasskoy-str-divizii-im-estonskogo-proletariata-novozybkov-1928#mode/inspect/page/18/zoom/4"})





os.path.getsize("books\\ГПИБ Вып. 1 Жилищное строительство в городских поселениях РСФСР, Украинской ССР и Белорусской ССР. - 1927\\00003.jpg")


import asyncio
from aiohttp import ClientSession,TCPConnector, ClientTimeout
async def fetch_image(url: str, i, headers_eph1, sem):
    """ Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)
    по url скачиваю картинку и добавляю в binary в queue asyncio
    """
    async with sem:
        async with ClientSession(headers=headers_eph1,timeout=ClientTimeout(total=5),trust_env=True) as session: #,trust_env=True
            async with session.get(url) as response:
                with open("New_Image_"+str(i)+".jpg","wb") as file:
                    file.write(await response.read())
                
async def async_images(urls,headers_eph1):
    """Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)
    call every tile image to download in async mode (in the end, add binary with the image number to results_prlDl
    """
    
    sem = asyncio.Semaphore(20)##https://stackoverflow.com/questions/63347818/aiohttp-client-exceptions-clientconnectorerror-cannot-connect-to-host-stackover
    tasks=[]
    for i in range(19):
        headers_eph1.update({'User-Agent': random.choice(user_agents)})
        tasks.append(asyncio.ensure_future(fetch_image(urls[i],i, headers_eph1,sem)))
    await asyncio.gather(*tasks)
#import nest_asyncio
#nest_asyncio.apply()
await async_images(urls, headers_eph1)


#return title, ext
def safe_file_name(value: str):
    if not value:
        return value
    value = re.sub(r'[^\w\s()\[\]{}.,-]+', ' ', value, flags=re.UNICODE)
    value = re.sub(r'[\s]+', ' ', value)
    value = value.strip(' \t.,')  # точка на конце запрещена в Windows
    return value


def mkdirs_for_regular_file(filename: str):
    """Создаёт все необходимые директории чтобы можно было записать указанный файл"""
    dirname = os.path.dirname(filename)
    if not os.path.exists(dirname):
        try:
            os.makedirs(dirname)
        except OSError as e:  # Guard against race condition
            if e.errno != errno.EEXIST:
                raise


def select_one_text_optional(root, selector: str):
    tag = root.select_one(selector)
    if not tag:
        raise Exception(f'Не найден элемент по пути {selector}')
    text = tag.text if tag else ''
    return text.strip()



user_agents = [
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36',
    'Mozilla/5.0 (Windows NT 5.1; rv:7.0.1) Gecko/20100101 Firefox/7.0.1',
    'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0'
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36'
]


eshplDl_params = {
    'quality': 8,
    'ext': 'jpg'
}


async def fetch_image_eshp1D1(url: str, headers_pr1, sem,img_path):
    """ Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)
    по url скачиваю картинку и добавляю в Файл 
    """
    flag=True
    #skip, if file exists:
    if os.path.isfile(img_path) and os.path.getsize(img_path)!=0:
        flag=False
    while flag: #check, so the size is ok:
        async with sem:
        
            async with ClientSession(headers=headers_pr1,timeout=ClientTimeout(total=30),trust_env=True) as session: #,trust_env=True
                  
                async with session.get(url) as response:
                    with open(img_path,"wb") as file:
                        file.write(await response.read())
                if os.path.getsize(img_path)!=0:
                    flag=False

async def async_images_eshp1D1(img_url_list,headers_eph1_list,image_path_list):
    """Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)
    call every tile image to download in async mode и автоматически скачать книги в папку
    """
    sem = asyncio.Semaphore(3)##https://stackoverflow.com/questions/63347818/aiohttp-client-exceptions-clientconnectorerror-cannot-connect-to-host-stackover
    tasks=[]
    for i in range(len(img_url_list)):
        
        tasks.append(asyncio.ensure_future(fetch_image_eshp1D1(img_url_list[i], headers_eph1_list[i],sem,image_path_list[i])))
    await asyncio.gather(*tasks)

        
def eshplDl(url):
    ext = eshplDl_params['ext']
    quality = eshplDl_params['quality']
    domain = urllib.parse.urlsplit(url).netloc
    global headers_eph1
    BOOK_DIR="books_test"
    headers_eph1.update({'User-Agent': random.choice(user_agents)})
    html_text = requests.get(url, headers=headers_eph1).text
    soup = BeautifulSoup(html_text, 'html.parser')
    title = select_one_text_optional(soup, 'title') or md5_hex(url)
    title = safe_file_name(title)
    
    for script in soup.find_all('script'):
        st = str(script)
        if 'initDocview' in st:
            book_json = json.loads(st[st.find('{"'): st.find(')')])
    logging.info(f' Каталог для загрузки: {title}')
    
    pages = book_json['pages']
    
    headers_eph1_list=[]
    image_path_list=[]
    img_url_list=[]
    for idx, page in enumerate(pages):
        img_url = f'http://{domain}/pages/{page["id"]}/zooms/{quality}'
        image_short = '%05d.%s' % (idx+1, ext)
        image_path = os.path.join(BOOK_DIR, title, image_short)
        mkdirs_for_regular_file(image_path)
        headers_eph1.update({'Referer': url})
        
        headers_eph1_list.append(headers_eph1)
        image_path_list.append(image_path)
        img_url_list.append(img_url)
    flag=True
    while flag:    
        try:
            import nest_asyncio
            nest_asyncio.apply()
            asyncio.run(async_images_eshp1D1(img_url_list, headers_eph1_list,image_path_list))
        except Exception as Argument:  #Error coding
                    time.sleep(1.0)
                    logging.exception("Error occurred in ASYNCIO") 
        else:
            lst = os.listdir(os.path.join(BOOK_DIR, title)) # your directory path
            
            if len(lst)==len(img_url_list):
                flag=False
import time
start=time.time()
eshplDl("http://elib.shpl.ru/ru/nodes/16533-vyp-1-zhilischnoe-stroitelstvo-v-gorodskih-poseleniyah-rsfsr-ukrainskoy-ssr-i-belorusskoy-ssr-1927")
print(time.time()-start)











import argparse
import json
import os
import re
import urllib.parse
import asyncio
from aiohttp import ClientSession,TCPConnector, ClientTimeout
import datetime
import time
import numpy as np
#import nest_asyncio #used for debugging
from util import CV2_Russian, BinaryToDecimal,number_of_images, Postprocess, Time_Processing,archive_ia, fetch_metadata
import cv2
import random
import img2pdf
from bs4 import BeautifulSoup
import sys
from util import get_logger
from util import md5_hex, to_float, cut_bom, perror, progress, ptext, safe_file_name, Browser, select_one_text_optional
from util import select_one_text_required, select_one_attr_required, gwar_fix_json,mkdirs_for_regular_file
from util import user_agents
import logging
import threading
import requests


user_agents = [
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36',
    'Mozilla/5.0 (Windows NT 5.1; rv:7.0.1) Gecko/20100101 Firefox/7.0.1',
    'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0'
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36'
]


headers_eph1 = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Encoding": "gzip, deflate, br, zstd",
    "Accept-Language": "en-US,en;q=0.9",
    "Connection": "keep-alive",
    "Dnt": "1",
    "Host": "httpbin.io",
    "Sec-Ch-Ua": '"Chromium";v="136", "Google Chrome";v="136", "Not.A/Brand";v="99"',
    "Sec-Ch-Ua-Mobile": "?0",
    "Sec-Ch-Ua-Platform": '"Windows"',
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "cross-site",
    "Sec-Fetch-User": "?1",
    "Sec-Gpc": "1",
    "Upgrade-Insecure-Requests": "1"
}





i=1
with open(f"urls55_{i}.txt", "w") as file:
    file.write('\n'.join(["1","2","3"])+"\n")


threads=[True, True, True]
Cores=3


not all([ threads[i]==True for i in range(Cores)])


file= open(f"urls55_{i}.txt", "r+")
print(file.read().splitlines())


import glob
new_title="Juridicheskoepolozhenielichnostivovremja"
if glob.glob('books\\'+new_title[:-2]+'*.zip'):
    print("a")


i=1
print(f'Thread {i} finished downloading')


with open("D:\\personal_data.txt","r") as file:
    session=file.read().splitlines()
session


description


from selenium.webdriver.common.by import By

from bs4 import BeautifulSoup
import requests

html_text =requests.get("https://www.prlib.ru/item/343041").text
soup = BeautifulSoup(html_text, 'html.parser')
authors=soup.find("ul",{"class":"field field-name-field-book-author field-type-taxonomy-term-reference field-label-hidden"})
author_=[]
for author in authors.find_all("a"):
    author_.append(author.text)

description=""
for desc in soup.find("div",{"class":"field field-name-field-book-bd field-type-text-long field-label-hidden"}).find_all("td")[2:][:-1]:
    description+=desc.get_text(strip=True).replace("\n","")+"\n"


metadata={
"creator" : author_,
"language" : "Russian",
"mediatype" : "texts",
"scanner" : "Internet Archive Python library 3.0.0",
"title" : title,
"description":   description
}


#rename everything in a folder + files: + #Upload
import os
import transliterate
import shutil
from random import randrange

title="Материалы для истории раскола за первое время его существования, издаваемые редакцией Братского слова . Т. 4. Историко- и догма"

new_title=transliterate.translit(title, reversed=True).replace(" ","")[:40]+"_"+str(randrange(20))
os.rename("books\\"+title, "books\\"+new_title)
root="books\\"+new_title
for dir, subdirs, files in os.walk(root):
    for f in files:
        f_new = new_title+ f
        os.rename(os.path.join(root, f), os.path.join(root, f_new))





new_name=root+"_images.zip"
os.rename(root+".zip", "books\\123Materialydljaistorii_15_images.zip")


shutil.make_archive(root,
                    'zip',
                    root)



#internetarchive.modify_metadata("123Materialydljaistorii_15",metadata,access_key=session[0], secret_key=session[1])


import internetarchive
internetarchive.upload("123Materialydljaistorii_15", "books\\123Materialydljaistorii_15_images.zip", metadata, verbose=True,queue_derive=True,access_key=session[0], secret_key=session[1])








from internetarchivepdf import recode
import pytesseract
from PIL import Image


pytesseract.pytesseract.tesseract_cmd = r'C:\Users\Lenovo\AppData\Local\Programs\Tesseract-OCR\tesseract'


# https://pypi.org/project/pytesseract/
print(pytesseract.image_to_string(Image.open('books\\spravochnik_images\\spravochnik_00020.jpg'), lang="rus"))


hocr = pytesseract.image_to_pdf_or_hocr('books\\spravochnik_images\\spravochnik_00020.jpg', extension='hocr')


with open("20.hocr","wb") as file:
    file.write(hocr)























url="https://www.prlib.ru/node/333266/source"


!python booklead.py --pdf y --url "https://www.prlib.ru/node/333266/source"


import requests
html_text=requests.get(url).text


from bs4 import BeautifulSoup 
import json
soup = BeautifulSoup(html_text, 'html.parser')
#sys.stdout.write(title)
#ptext(f'Каталог для загрузки: {title}')


import requests
import os
for i in range(1,117):
    url="https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF=/var/data/scans/public/9216FE9E-4343-4ECA-AE4A-04FF759FBF4B/5079621/5079622_doc1_E033BDF8-CAD3-43C8-94B1-0230154A45D4.tiff&JTL=4,{}".format(i)
    print(url)
    res=requests.get(url)
    import shutil
    with open(os.path.join("Images","new_image{}.jfif".format(i)), "wb") as fd:
        fd.write(res.content)


url="https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF=/var/data/scans/public/9216FE9E-4343-4ECA-AE4A-04FF759FBF4B/5079621/5079622_doc1_E033BDF8-CAD3-43C8-94B1-0230154A45D4.tiff&JTL=4,{}".format(0) 
res=requests.get(url)
img = cv2.imdecode(np.frombuffer(res.content,np.uint8),-1)
img = cv2.cvtColor(img,cv2.IMREAD_COLOR)


cv2.imshow('sea_image.jfif', img )
cv2.waitKey(0)
cv2.destroyAllWindows()
cv2.waitKey(1)





import asyncio
from aiohttp import ClientSession
import json
import os
import numpy as np
import time
import asyncio
import cv2
import shutil

start=time.time()


async def fetch_image(url: str, i):
    async with ClientSession() as session:
        async with session.get(url) as response:
            result = await response.read()
            with open(os.path.join("Test", "binary"+str(i)+".jpg"), "wb") as file:
                file.write(result)


async def async_images(url,num ):
    async with asyncio.TaskGroup() as group:
        
        for i in range(num):
            group.create_task(fetch_image(url.format(i), i))

def stitch_images(num):
    new_img=[]
    new_line=False
    for i in range(num):
        img1=cv2.imread(os.path.join("Test", "binary"+str(i)+".jpg"))
        if i==0 or new_line:
            im_h = img1
            new_line=False
        else:
            im_h = cv2.hconcat([im_h, img1])
        if i!=0 and len(img1[0])!=256:
            new_line=True
            if len(new_img)==0:
                new_img=im_h
            else:
                new_img=cv2.vconcat([new_img, im_h])

    cv2.imwrite('output_image.jpg', new_img)
def number_of_images(width, height):
    num_w=width//256
    if width%256!=0:
        num_w+=1
    num_h=height//256
    if height%256!=0:
        num_h+=1
    return num_w*num_h

url="https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF=/var/data/scans/public/9216FE9E-4343-4ECA-AE4A-04FF759FBF4B/5079621/5079626_doc1_CCDC9C46-9C5C-4F7B-BD01-2DB11FAE6B80.tiff&JTL="
url=url+str(len(page['d']))+","+"{}"
num=number_of_images(page["d"][len(page['d']) - 1]['w'],page["d"][len(page['d']) - 1]['h'])
await async_images(url,num)
stitch_images(num)
#delete Help folder

print(time.time()-start)


stitch_images()


new=cv2.imread(os.path.join("Test", "binary"+str(1)+".jpg"))
cv2.imshow("new.jpg", new)
cv2.waitKey(0)
cv2.destroyAllWindows()
cv2.waitKey(1)


len(Total_image)


new_img=Total_image[0]
for j in range(1,len(Total_image)):

    new_img=cv2.vconcat([new_img, Total_image[j]])
cv2.imshow('sea_image.jfif', cv2.resize(new_img, dsize=(0,0), fx=0.5, fy=0.5) )
cv2.waitKey(0)
cv2.destroyAllWindows()
cv2.waitKey(1)





user_agents = [
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36',
    'Mozilla/5.0 (Windows NT 5.1; rv:7.0.1) Gecko/20100101 Firefox/7.0.1',
    'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0'
]
import random


import asyncio
from aiohttp import ClientSession
import json
import os
import numpy as np
import time
import asyncio
import cv2
import shutil
import nest_asyncio

start=time.time()
results_prlDl=[]

async def fetch_image(url: str, i,queue,headers):
    """ Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)
    по url скачиваю картинку и добавляю в binary в queue asyncio
    """
    async with ClientSession() as session:
        async with session.get(url, headers=headers) as c:
            with open(os.path.join("Test", "binary"+str(i)+".jpg"), "wb") as file:
                file.write(await response.read())
            
            #result = (i, await response.read())
            #await queue.put(result)
           
async def async_images(url,num):
    """Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)
    call every tile image to download in async mode (in the end, add binary with the image number to results_prlDl
    """
    global results_prlDl
    results_prlDl=[]
    
    queue = asyncio.Queue()
    headers={'Referer': url}
    headers.update({'User-Agent': random.choice(user_agents)})
    async with asyncio.TaskGroup() as group:
        for i in range(num):
            group.create_task(fetch_image(url.format(i), i,queue,headers))
    while not queue.empty():
        results_prlDl.append(await queue.get())

def number_of_images(width, height):
    num_w=width//256
    if width%256!=0:
        num_w+=1
    num_h=height//256
    if height%256!=0:
        num_h+=1
    return num_w,num_h    
def BinaryToDecimal(binary):
    with open("test.jpg", "wb") as file:
        file.write(binary)
    dec=cv2.imread("test.jpg")
    os.remove("test.jpg")
    return dec

def Postprocess(results_prlDl, height):
    Total_Image=[_ for _ in range(len(results_prlDl))]
    for item in results_prlDl:
        Total_Image[item[0]]=BinaryToDecimal(item[1])
    regroup=[]
    for h in range(height):
        regroup.append(Total_Image[h*width:(h+1)*width])
    im_h=cv2.vconcat([cv2.hconcat(item) for item in regroup])
    cv2.imwrite('output_image.jpg', im_h)


image_name="5079628_doc1_805022FF-DDBF-4965-9657-E6A30B4BD0A4.tiff"
url="https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF=/var/data/scans/public/9216FE9E-4343-4ECA-AE4A-04FF759FBF4B/5079621/"+image_name+"&JTL="
width, height=number_of_images(page["d"][len(page['d']) - 1]['w'],page["d"][len(page['d']) - 1]['h'])
url=url+str(len(page['d']))+","+"{}"
flag=True
while flag:
    try:
        nest_asyncio.apply()
        asyncio.run(async_images(url,width*height))
    except:
        continue
    else:
        flag=False

#asyncio.run(main()) 
print(results_prlDl)
Postprocess(results_prlDl,height)

print(time.time()-start)


name="D:\\ANDREY_BOOKS\\Russian libs\\BOOKS_Andrey\\booklead-master\\books\\"+stra+"\\new.jpg"
f = open(name, "rb")
chunk = f.read()
chunk_arr = np.frombuffer(chunk, dtype=np.uint8)
img = cv2.imdecode(chunk_arr, cv2.IMREAD_COLOR)
result, data = cv2.imencode('.jpg', img)
fh = open('лалалалал.jpg', 'wb')
fh.write(data)
fh.close()


result, data = cv2.imencode('.jpg', img)


stra="АЛАЛАЛАЛАЛАЛАЛА"


with open("new.jpg", "rb") as file:
    binary=file.read()
    cv2.imread(


page={"d": [
        {
          "h": 392,
          "w": 272
        },
        {
          "h": 785,
          "w": 544
        },
        {
          "h": 1570,
          "w": 1088
        },
        {
          "h": 3140,
          "w": 2176
        }
      ]}


import json
with open('eltsin.json') as f:
    json1 = json.load(f)






"item" in "https://www.prlib.ru/item/469492".split("prlib.ru/")[1]


# -*- coding: utf-8 -*-
import argparse
import json
import os
import re
import urllib.parse
import asyncio
from aiohttp import ClientSession
import datetime
import numpy as np
import nest_asyncio
from util import CV2_Russian, BinaryToDecimal,number_of_images, Postprocess, Time_Processing
import cv2
import random
import img2pdf
from bs4 import BeautifulSoup
import sys
from util import get_logger
from util import md5_hex, to_float, cut_bom, perror, progress, ptext, safe_file_name, Browser, select_one_text_optional
from util import select_one_text_required, select_one_attr_required, gwar_fix_json,mkdirs_for_regular_file
from util import user_agents

log = get_logger(__name__)

BOOK_DIR = 'books'

eshplDl_params = {
    'quality': 8,
    'ext': 'jpg'
}

prlDl_params = {
    'ext': 'jpg' }
 

bro: Browser


def makePdf(pdf_path, img_folder, img_ext):
    img_list = []
    for r, _, ff in os.walk(img_folder):
        for fname in ff:
            if fname.endswith(f'.{img_ext}'):
                img_list.append(os.path.join(r, fname))
    img_list.sort()
    pdf = img2pdf.convert(img_list)
    with open(pdf_path, "wb") as fd:
        fd.write(pdf)


def saveImage(url, img_id, folder, ext, referer):
    image_short = '%05d.%s' % (img_id, ext)
    image_path = os.path.join(BOOK_DIR, folder, image_short)
    headers = {'Referer': referer}
    expected_ct = re.compile('image/')
    bro.download(url, image_path, headers, content_type=expected_ct, skip_if_file_exists=True)



async def fetch_image(url: str, i,queue):
    """ Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)
    по url скачиваю картинку и добавляю в binary в queue asyncio
    """
    async with ClientSession() as session:
        async with session.get(url) as response:
            result = (i, await response.read())
            await queue.put(result)
def exception_handler(loop, context):
	# get the exception object
	exp = context['exception']
	# log exception
	logging.error(f'Never-retrieved exception: {exp}')     
async def async_images(url,num,headers):
    """Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)
    call every tile image to download in async mode (in the end, add binary with the image number to results_prlDl
    """

    
    queue = asyncio.Queue()
    #headers = headers
    #headers.update({'User-Agent': random.choice(user_agents)})
    async with asyncio.TaskGroup() as group:
        for i in range(num):
            group.create_task(fetch_image(url.format(i), i,queue))
    global results_prlDl
    results_prlDl=[]
    while not queue.empty():
        results_prlDl.append(await queue.get())


def eshplDl(url):
    ext = eshplDl_params['ext']
    quality = eshplDl_params['quality']
    domain = urllib.parse.urlsplit(url).netloc

    html_text = bro.get_text(url)
    
    soup = BeautifulSoup(html_text, 'html.parser')
    title = select_one_text_optional(soup, 'title') or md5_hex(url)
    title = safe_file_name(title)
    
    for script in soup.findAll('script'):
        
        
        st = str(script)
        
        if 'initDocview' in st:
            book_json = json.loads(st[st.find('{"'): st.find(')')])
    ptext(f' Каталог для загрузки: {title}')
    pages = book_json['pages']
    for idx, page in enumerate(pages):
        img_url = f'http://{domain}/pages/{page["id"]}/zooms/{quality}'
        saveImage(img_url, idx + 1, title, ext, url)
        progress(f'  Прогресс: {idx + 1} из {len(pages)} стр.')
    return title, ext
    

def prlDl(url):
    """
    Президентская библиотека имени Б.Н. Ельцина
    Формат - серия изображений
    Пример урла книги (HTML) - https://www.prlib.ru/item/420931
    """
    ext = prlDl_params['ext']
    html_text = bro.get_text(url)
    soup = BeautifulSoup(html_text, 'html.parser')
    title = select_one_text_optional(soup, 'h1') or md5_hex(url)
    title = safe_file_name(title)
    ptext(f'Каталог для загрузки: {title}')
    
    for script in soup.find_all('script'): #findAll deprecated
        st = str(script)
        if 'jQuery.extend' in st:
            book_json = json.loads(st[st.find('{"'): st.find(');')])
            if "item" in url.split("prlib.ru/")[1]:   #case for https://www.prlib.ru/item/*** 
                book = book_json['diva']['1']['options']
            elif "node" in url.split("prlib.ru/")[1]:     #case for https://www.prlib.ru/node/***
                book = book_json['diva']['settings']
    json_text = bro.get_text(book['objectData'])
    book_data = json.loads(json_text)
    pages = book_data['pgs']
    num_of_pages_down=0 #for the time prediction
    start=datetime.datetime.now()#for the time prediction
    headers = {'Referer': url}
    for idx, page in enumerate(pages):
        
        img_url = 'https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF={}/{}&JTL={},'.format(
            book['imageDir'], page['f'], page['m']) #поменял здесь немного вид урл, так как по частям качаю
        # брал урл отсюда: https://iipimage.sourceforge.io/documentation/protocol
        img_url+="{}"
        width, height=number_of_images(page["d"][len(page['d']) - 1]['w'],page["d"][len(page['d']) - 1]['h'])
        
        image_short = '%05d.%s' % (idx+1, ext)
        image_path = os.path.join(BOOK_DIR, title, image_short)

        # заменяю все фичи ручками (например тут skip_if_exists), которые были ранее доступны через функции 
        #(т.к. метод у меня скачивания немного другой)
        if os.path.exists(image_path) and os.stat(image_path).st_size > 0:
            log.info(f'Пропускаю скачанный файл: {image_path}')
            progress(f'  Прогресс: {idx + 1} из {len(pages)} стр. ')
        else: 
            mkdirs_for_regular_file(image_path)
            #nest_asyncio.apply() # нужен только чтобы async работал нормально в Jupyter ( https://pypi.org/project/nest-asyncio/)
            # получить все данные с картиники
            try:
                loop=asyncio.get_event_loop()
                task=asyncio.Task(async_images(img_url,width*height,headers))
                loop.set_exception_handler(exception_handler)
                loop.run_until_complete(task)
            
                #asyncio.run(async_images(img_url,width*height,headers))
            except Exception as e:
                # log the exception
                logging.exception(f'Failed with exception: {e}')
            global results_prlDl
            # просессить все данные и в конце вывести картинку
            Postprocess(results_prlDl,width,height, image_path)
            
            # Time Formatting/Prediction:
            prog=datetime.datetime.now()-start
            num_of_pages_down+=1
            left=prog/num_of_pages_down*(len(pages)-(idx+1)) #based on the values before prediction
            minutes, seconds = Time_Processing(left)
            past_min, past_sec=Time_Processing(prog)
            progress(f'  Прогресс: {idx + 1} из {len(pages)} стр. | Прошло (мин:сек): {past_min}:{past_sec:02d} ;Осталось: {minutes}:{seconds:02d} ')
    return title, ext


def unatlib_download(url):
    """
    Национальная электронная библиотека Удмуртской республики
    Формат - PDF
    Пример урла книги (HTML) - https://elibrary.unatlib.ru/handle/123456789/18116
    Пример урла книги (PDF) - https://elibrary.unatlib.ru/bitstream/handle/123456789/18116/uiiyl_book_075.pdf
    Реферером должен быть https://elibrary.unatlib.ru/build/pdf.worker.js
    """
    html_text = bro.get_text(url)
    soup = BeautifulSoup(html_text, 'html.parser')
    title = select_one_text_required(soup, 'title') or md5_hex(url)
    title = safe_file_name(title)
    pdf_href = select_one_attr_required(soup, '#dsview', 'href')
    pdf_url = f'https://elibrary.unatlib.ru{pdf_href}'
    headers = {'Referer': 'https://elibrary.unatlib.ru/build/pdf.worker.js'}
    pdf_file = os.path.join(BOOK_DIR, f'{title}.pdf')
    bro.download(pdf_url, pdf_file, headers, skip_if_file_exists=True)
    return None  # all done, no further action needed


def gwarDL(url): 
    """
    Первая мировая война 1914-1918 - Информационный портал
    Формат - серия изображений/PDF
    Пример урла type 1 (HTML) - https://gwar.mil.ru/heroes/document/50000001/
    Пример урла type 2 (HTML) - https://gwar.mil.ru/documents/view/?id=88000899 или https://gwar.mil.ru/documents/view/88009650/
    Пример урла type 3 (PDF) - https://gwar.mil.ru/books/105501406
    """
    ext = 'jpg' # пока так
    json_url = ''
    request_data = {}

    html_text = bro.get_text(url)
    soup = BeautifulSoup(html_text, 'html.parser')

    title = select_one_text_required(soup, 'title') or md5_hex(url)
    title = safe_file_name(title)

    for script in soup.findAll('script'):
        st = str(script)
        if 'var parentId' in st: # type 1
            page_json = st[st.find('{'): st.find(';\n</')]
            page_json_fix = gwar_fix_json(page_json, True)
            book_id = page_json_fix['id']
            boxes_id = page_json_fix['documents_pages']['deals_boxes_id']

            request_data = {
                "indices": ["gwar"],
                "entities": ["stranitsa"],
                "queryFields": {
                    "deal_box_id": boxes_id
                    },
                "from": 0,
                "size": 3000,
                "builderType": "HeroesStranitsa"
                }
            
            json_url = 'https://gwar.mil.ru/gt_data/?builder=HeroesStranitsa'
        elif 'var documentjs' in st: # type 2
            page_json = st[st.find('{\''): st.find('</script>')]
            page_json_fix = gwar_fix_json(page_json)
            book_id = page_json_fix['id']

            if (page_json_fix['hits']['hits'][0]['_type'] == 'document'):
                query_fields = {
                    "document_id": book_id,
                    }
            elif (page_json_fix['hits']['hits'][0]['_type'] == 'deal'):
                query_fields = {
                    "document_id": book_id,
                    "deal_box_id": book_id
                    }

            request_data = {
                "indices": "gwar_document",
                "entities": "document_image",
                "queryFields": query_fields,
                "from": 0,
                "size": 10000,
                "builderType": "DocumentView"
                }

            json_url = 'https://gwar.mil.ru/gt_data/?builder=DocumentView'
        elif 'window.$.fn.initDetailBook();' in st: # type 3
            for item in soup.find_all(attrs={"data-id": True}):
                pdf_href = item['data-id']
                pdf_url = f'https://cdn.gwar.mil.ru/bookload/{pdf_href}.pdf'
                headers = {'Referer': url}
                pdf_file = os.path.join(BOOK_DIR, f'{title}.pdf')
                bro.download(pdf_url, pdf_file, headers, skip_if_file_exists=True)
            return None  # all done, no further action needed

    book_dir = ('{}_{}'.format(book_id, title))[0:224]

    ptext(f' Каталог для загрузки: {book_dir}')
    request_headers = {'referer': url}

    json_text = bro.post_text(json_url, request_headers, request_data)
    book_data = json.loads(json_text)
    pages = book_data['hits']['hits']
    for idx, page in enumerate(pages):
        if (page['_type'] == 'document_image'):
            image_url = page['_source']['path']
        elif (page['_type'] == 'stranitsa'):
            image_url = page['_source']['obraz_s_oblastyami']

        if (image_url.find('<i src="') >= 0):
            regexp = re.compile(r'<i src="(\S*?)"')
            if regexp.findall(image_url): 
                image_url = regexp.findall(image_url)[0]

        img_url = 'https://cdn.gwar.mil.ru/imagesfww/{}'.format( # либо ...ru/imageloadfull/
            image_url)
        saveImage(img_url, idx + 1, book_dir, ext, 'https://gwar.mil.ru/')
        progress(f'  Прогресс: {idx + 1} из {len(pages)} стр.')
    return title, ext


domains = {
    'elib.shpl.ru': eshplDl,
    'docs.historyrussia.org': eshplDl,
    'prlib.ru': prlDl,
    'www.prlib.ru': prlDl,
    'elibrary.unatlib.ru': unatlib_download,
    'gwar.mil.ru': gwarDL
}


def download_book(url):
    try:
        log.info(f'Скачиваю книгу {url}')
        host = urllib.parse.urlsplit(url)
        if not host.hostname:
            perror(f'Некорректный урл: {url}')
            return None
        site_downloader = domains.get(host.hostname)
        if not site_downloader:
            perror(f'Домен {host.hostname} не поддерживается')
            return None
        ptext(f'Cсылка: {url}')
        return site_downloader(url)
    except Exception as e:
        log.exception('Перехвачена ошибка в download_book')
        perror(e)
        return None


def collect_urls():
    urls = []
    if args.url:
        urls.append(args.url)
    if args.list:
        with open(args.list) as fp:
            urls.extend([line.strip() for line in fp])
    return list(
        filter(lambda x: not x.startswith('#'),
               filter(bool,
                      map(lambda x: cut_bom(x).strip(), urls))))


def main():
    try:
        global bro
        log.info('Программа стартовала')
        urls = collect_urls()
        ptext(f'Ссылок для загрузки: {len(urls)}')
        pause = 0
        if args.pause:
            pause = to_float(args.pause)
        bro = Browser(pause=pause)
        for url in urls:
            load = download_book(url)
            if load and args.pdf.lower() in ['y', 'yes']:
                progress('  Создание PDF...')
                title, img_ext = load
                img_folder_full = os.path.join(BOOK_DIR, title)
                pdf_path = os.path.join(BOOK_DIR, f'{title}.pdf')
                makePdf(pdf_path, img_folder_full, img_ext)
                ptext(f' - Файл сохранён: {pdf_path}')
    except KeyboardInterrupt:
        perror('\nЗагрузка прервана пользователем')
    except Exception as e:
        log.exception('\nПерехвачена ошибка в main')
        perror(e)
    finally:
        log.info('\nПрограмма завершена')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='booklead - Загрузчик книг из интернет-библиотек')
    parser.add_argument('--pdf', dest='pdf', default='', metavar='y', help='Создавать PDF-версии книг')
    parser.add_argument('--list', dest='list', default='', metavar='"list.txt"', help='Файл со списком книг')
    parser.add_argument('--url', dest='url', default='', metavar='"http://..."', help='Ссылка на книгу')
    parser.add_argument('--pause', dest='pause', default='0', metavar='1.0',
                        help='Пауза между HTTP-запросами в секундах')
    args = parser.parse_args()
    if args.url or args.list:
        main()
    else:
        parser.print_help()



prlDl("https://www.prlib.ru/node/333266/source")


# -*- coding: utf-8 -*-
import argparse
import json
import os
import re
import urllib.parse
import asyncio
from aiohttp import ClientSession
import datetime
import numpy as np
import nest_asyncio
from util import CV2_Russian, BinaryToDecimal,number_of_images, Postprocess, Time_Processing
import cv2
import random
import img2pdf
from bs4 import BeautifulSoup
import sys
from util import get_logger
from util import md5_hex, to_float, cut_bom, perror, progress, ptext, safe_file_name, Browser, select_one_text_optional
from util import select_one_text_required, select_one_attr_required, gwar_fix_json,mkdirs_for_regular_file
from util import user_agents

log = get_logger(__name__)

BOOK_DIR = 'books'

eshplDl_params = {
    'quality': 8,
    'ext': 'jpg'
}

prlDl_params = {
    'ext': 'jpg' }
 

bro: Browser


def makePdf(pdf_path, img_folder, img_ext):
    img_list = []
    for r, _, ff in os.walk(img_folder):
        for fname in ff:
            if fname.endswith(f'.{img_ext}'):
                img_list.append(os.path.join(r, fname))
    img_list.sort()
    pdf = img2pdf.convert(img_list)
    with open(pdf_path, "wb") as fd:
        fd.write(pdf)


def saveImage(url, img_id, folder, ext, referer):
    image_short = '%05d.%s' % (img_id, ext)
    image_path = os.path.join(BOOK_DIR, folder, image_short)
    headers = {'Referer': referer}
    expected_ct = re.compile('image/')
    bro.download(url, image_path, headers, content_type=expected_ct, skip_if_file_exists=True)



async def fetch_image(url: str, i,queue, headers):
    """ Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)
    по url скачиваю картинку и добавляю в binary в queue asyncio
    """
    async with ClientSession(headers=headers) as session:
        async with session.get(url) as response:
            result = (i, await response.read())
            await queue.put(result)
            
async def async_images(url,num,headers):
    """Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)
    call every tile image to download in async mode (in the end, add binary with the image number to results_prlDl
    """

    queue = asyncio.Queue()
    async with asyncio.TaskGroup() as group:
        for i in range(num):
            group.create_task(fetch_image(url.format(i), i,queue,headers))
    global results_prlDl
    results_prlDl=[]
    while not queue.empty():
        results_prlDl.append(await queue.get())


def eshplDl(url):
    ext = eshplDl_params['ext']
    quality = eshplDl_params['quality']
    domain = urllib.parse.urlsplit(url).netloc

    html_text = bro.get_text(url)
    
    soup = BeautifulSoup(html_text, 'html.parser')
    title = select_one_text_optional(soup, 'title') or md5_hex(url)
    title = safe_file_name(title)
    
    for script in soup.findAll('script'):
        
        
        st = str(script)
        
        if 'initDocview' in st:
            book_json = json.loads(st[st.find('{"'): st.find(')')])
    ptext(f' Каталог для загрузки: {title}')
    pages = book_json['pages']
    for idx, page in enumerate(pages):
        img_url = f'http://{domain}/pages/{page["id"]}/zooms/{quality}'
        saveImage(img_url, idx + 1, title, ext, url)
        progress(f'  Прогресс: {idx + 1} из {len(pages)} стр.')
    return title, ext
    

def prlDl(url):
    """
    Президентская библиотека имени Б.Н. Ельцина
    Формат - серия изображений
    Пример урла книги (HTML) - https://www.prlib.ru/item/420931
    """
    global bro
    bro = Browser(pause=0)
    ext = prlDl_params['ext']
    html_text = bro.get_text(url)
    soup = BeautifulSoup(html_text, 'html.parser')
    title = select_one_text_optional(soup, 'h1') or md5_hex(url)
    title = safe_file_name(title)
    ptext(f'Каталог для загрузки: {title}')
    
    for script in soup.find_all('script'): #findAll deprecated
        st = str(script)
        if 'jQuery.extend' in st:
            book_json = json.loads(st[st.find('{"'): st.find(');')])
            if "item" in url.split("prlib.ru/")[1]:   #case for https://www.prlib.ru/item/*** 
                book = book_json['diva']['1']['options']
            elif "node" in url.split("prlib.ru/")[1]:     #case for https://www.prlib.ru/node/***
                book = book_json['diva']['settings']
    json_text = bro.get_text(book['objectData'])
    book_data = json.loads(json_text)
    pages = book_data['pgs']
    num_of_pages_down=0 #for the time prediction
    start=datetime.datetime.now()#for the time prediction
    headers = {'Referer': url}
    for idx, page in enumerate(pages):
        
        img_url = 'https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF={}/{}&JTL={},'.format(
            book['imageDir'], page['f'], page['m']) #поменял здесь немного вид урл, так как по частям качаю
        # брал урл отсюда: https://iipimage.sourceforge.io/documentation/protocol
        img_url+="{}"
        width, height=number_of_images(page["d"][len(page['d']) - 1]['w'],page["d"][len(page['d']) - 1]['h'])
        
        image_short = '%05d.%s' % (idx+1, ext)
        image_path = os.path.join(BOOK_DIR, title, image_short)

        # заменяю все фичи ручками (например тут skip_if_exists), которые были ранее доступны через функции 
        #(т.к. метод у меня скачивания немного другой)
        if os.path.exists(image_path) and os.stat(image_path).st_size > 0:
            log.info(f'Пропускаю скачанный файл: {image_path}')
            progress(f'  Прогресс: {idx + 1} из {len(pages)} стр. ')
        else: 
            mkdirs_for_regular_file(image_path)
            nest_asyncio.apply() # нужен только чтобы async работал нормально в Jupyter ( https://pypi.org/project/nest-asyncio/)
            # получить все данные с картиники
            headers.update({'User-Agent': random.choice(user_agents)})
            try:
                asyncio.run(async_images(img_url,width*height,headers))
            except Exceptions as e:
                print(e)
            global results_prlDl
            # просессить все данные и в конце вывести картинку
            Postprocess(results_prlDl,width,height, image_path)
            
            # Time Formatting/Prediction:
            prog=datetime.datetime.now()-start
            num_of_pages_down+=1
            left=prog/num_of_pages_down*(len(pages)-(idx+1)) #based on the values before prediction
            minutes, seconds = Time_Processing(left)
            past_min, past_sec=Time_Processing(prog)
            progress(f'  Прогресс: {idx + 1} из {len(pages)} стр. | Прошло (мин:сек): {past_min}:{past_sec:02d} ;Осталось: {minutes}:{seconds:02d} ')
    return title, ext


prlDl("https://www.prlib.ru/item/355048")





import cv2 
#img1 = cv2.imread('iipsrv (1).jfif') 
#img2 = cv2.imread('iipsrv (2).jfif')
#cv2.imshow("Image 1", img1)
#im_h = cv2.hconcat([img1, img2]) 
cv2.imshow('sea_image.jfif', img) 
cv2.waitKey(0)
cv2.destroyAllWindows()
cv2.waitKey(1)

