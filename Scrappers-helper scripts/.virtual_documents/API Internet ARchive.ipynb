





from internetarchive import get_session
import json
day='5'
c = {'s3': {'access': 'qJaX9KKXhXkzoN5o', 'secret': 'mmI4XUkxM9O8gZ15'}}
s = get_session(config=c)
query='uploader:"pavelserebrjanyi@gmail.com" AND mediatype:texts'
items=s.search_items(query, fields=["source_url"],request_kwargs={'timeout':40},params={"page":1, "rows":30000})



count=0
data=[]
for item in items:
    data.append([item["identifier"],item["source_url"]])


import csv
with open("duplic1.csv","w",newline='') as csvf:
    writer=csv.writer(csvf)
    writer.writerows(data)

#AFTER THAT EXCEL! to show only the 2nd occurence!


with open("dupl.txt","r") as file:
    BAD_IDS=file.read().splitlines()

with open("DU.txt","r") as file1:
    BAD=file1.read().splitlines()
new=[]
for ite in BAD:
    if ite not in BAD_IDS:
        new.append(ite)
print(new)


from internetarchive import modify_metadata

for bad in new:
    modify_metadata(bad,metadata={"TO_DELETE":1},access_key="qJaX9KKXhXkzoN5o",secret_key ="mmI4XUkxM9O8gZ15")








import csv
datafile="PrLib_Dataset_FULL_metadata.csv"
with open(datafile,"r", encoding='windows-1251') as csvfile: #read the place, where to put value
    f = csv.reader(csvfile)
    data=list(f)


data


data[0].append("Subject")
data[0].append("Description")
data[0].append("Creator")
urls=[row[2] for row in data]


from internetarchive import get_session
import json
day='5'
c = {'s3': {'access': 'qJaX9KKXhXkzoN5o', 'secret': 'mmI4XUkxM9O8gZ15'}}
s = get_session(config=c)
query='uploader:"pavelserebrjanyi@gmail.com" AND mediatype:texts'
items=s.search_items(query, fields=["subject", "description","source_url","creator"],request_kwargs={'timeout':40},params={"page":1, "rows":30000})


    


import time
start=time.time()
count=0
data_ia=[]
for item in items:
    print(count)
    data_upd={}
    data_upd.update({"source_url":item["source_url"]})
    if "subject" in item:
        data_upd.update({"subject":item["subject"]})
    if "description" in item:
        data_upd.update({"description":item["description"]})
    if "creator" in item:
        data_upd.update({"creator":item["creator"]})
    data_ia.append(data_upd)
    count+=1
print(time.time()-start)


count=0
for item in data_ia:
    print(count)
    if item["source_url"] in urls:
        ind=urls.index(item["source_url"])
        #change in this place data:
        try:
            data[ind].append(";".join(item["subject"]))  #subject
            
        except:
            data[ind].append("")
        try:
            data[ind].append(item["description"]) #description
        except:
            data[ind].append("")
        try:
            data[ind].append(";".join(item["creator"]))  #description
        except:
            data[ind].append("")
    count+=1








import asyncio
import aiohttp
from aiohttp import ClientSession
async def process(url,iden,idx,session,sem):
    async with sem:
        async with session.get(url) as response:
            resp=json.loads(await response.text())
            if resp[iden]['have_data']:
                print(iden)
                print()
            print(idx)
    
async def manager(identifiers):
    tasks=[]
    sem = asyncio.Semaphore(100)
    async with ClientSession() as session:
        for idx,iden in enumerate(identifiers):
            url='https://be-api.us.archive.org/views/v1/short/'
            url+=iden
            tasks.append(process(url,iden,idx,session,sem))
        await asyncio.gather(*tasks)


import nest_asyncio
nest_asyncio.apply()
asyncio.run(manager(identifiers))


resp







