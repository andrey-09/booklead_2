{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0cda6f-5dc3-4520-9284-7e37505028d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Python/3.12 aiohttp/3.9.5\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-6846c133-04d9e337706758b966a5e6df\"\n",
      "  }, \n",
      "  \"origin\": \"141.54.154.165\", \n",
      "  \"url\": \"http://httpbin.org/get\"\n",
      "}\n",
      "\n",
      "<CIMultiDictProxy('Date': 'Mon, 09 Jun 2025 11:10:43 GMT', 'Content-Type': 'application/json', 'Content-Length': '314', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true')>\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get('http://httpbin.org/get') as resp:\n",
    "            print(resp.status)\n",
    "            print(await resp.text())\n",
    "            print(resp.headers)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5d2cac-8d16-446e-aedd-769e3695f0d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m resp\u001b[38;5;241m.\u001b[39mheaders\n",
      "\u001b[1;31mNameError\u001b[0m: name 'resp' is not defined"
     ]
    }
   ],
   "source": [
    "resp.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279520f-b725-40d0-8552-41b6f6e772c2",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0bf6a7-6db7-4b94-ba9a-f65f0f5206e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f671caca-9d5f-41b2-b14e-6558c761a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import codecs\n",
    "import datetime\n",
    "import errno\n",
    "import functools\n",
    "import hashlib\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "import requests\n",
    "import threading\n",
    "from bs4 import Tag\n",
    "from requests import Response\n",
    "from typing import Dict, Optional, Pattern, Union\n",
    "import json\n",
    "from random import randrange\n",
    "import csv\n",
    "import transliterate \n",
    "import internetarchive\n",
    "from internetarchive.session import ArchiveSession\n",
    "from internetarchive import search_items\n",
    "from langdetect import detect #language detection\n",
    "\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import asyncio\n",
    "from aiohttp import ClientSession,TCPConnector, ClientTimeout\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "#import nest_asyncio #used for debugging\n",
    "from util import CV2_Russian, number_of_images, Postprocess, Time_Processing,archive_ia, fetch_metadata, CheckArchiveForWrites\n",
    "import cv2\n",
    "import random\n",
    "import img2pdf\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from util import get_logger\n",
    "from util import md5_hex, to_float, cut_bom, perror, progress, ptext, safe_file_name, Browser, select_one_text_optional\n",
    "from util import select_one_text_required, select_one_attr_required, gwar_fix_json,mkdirs_for_regular_file\n",
    "from util import user_agents\n",
    "from user_agent import generate_user_agent\n",
    "import logging\n",
    "import threading\n",
    "import requests\n",
    "\n",
    "\n",
    "log = get_logger(__name__)\n",
    "BOOK_DIR = 'test'\n",
    "\n",
    "eshplDl_params = {\n",
    "    'quality': 8,\n",
    "    'ext': 'jpg'\n",
    "}\n",
    "\n",
    "prlDl_params = {\n",
    "    'ext': 'jpg' }\n",
    "\n",
    "headers_pr1 = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br, zstd\", \n",
    "    \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8,ru;q=0.7\",\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "    'If-Modified-Since': 'Tue, 20 Dec 2016 02:17:59 GMT',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Sec-Gpc':'1',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',\n",
    "    'dnt': '1',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"137\", \"Google Chrome\";v=\"137\", \"Not/A)Brand\";v=\"24\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-gpc': '1',\n",
    "    'Host':'content.prlib.ru',\n",
    "    'Origin':'https://content.prlib.ru'\n",
    "}\n",
    "headers_pr2=headers_pr1 \n",
    "headers_pr2.update({\"Host\":\"www.prlib.ru\",\"Origin\": \"https://www.prlib.ru\"})\n",
    "headers_eph1 = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Dnt\": \"1\",\n",
    "    \"Host\": \"httpbin.io\",\n",
    "    \"Sec-Ch-Ua\": '\"Chromium\";v=\"136\", \"Google Chrome\";v=\"136\", \"Not.A/Brand\";v=\"99\"',\n",
    "    \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "    \"Sec-Ch-Ua-Platform\": '\"Windows\"',\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"cross-site\",\n",
    "    \"Sec-Fetch-User\": \"?1\",\n",
    "    \"Sec-Gpc\": \"1\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\"\n",
    "}\n",
    "\n",
    "bro: Browser\n",
    "bro=Browser(pause=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34460d4f-417d-4775-888c-f5709b150640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.111 Safari/537.36'}\n",
      "{'User-Agent': 'Mozilla/5.0 (Windows NT 5.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.111 Safari/537.36'}\n"
     ]
    }
   ],
   "source": [
    "from util import Browser\n",
    "bro=Browser(pause=1)\n",
    "url=\"https://www.prlib.ru/item/425843\"\n",
    "html_text = bro.get_text(url)\n",
    "soup = BeautifulSoup(html_text, 'html.parser')\n",
    "for script in soup.find_all('script'): #findAll deprecated\n",
    "    st = str(script)\n",
    "    if 'jQuery.extend' in st:\n",
    "        book_json = json.loads(st[st.find('{\"'): st.find(');')])\n",
    "        \n",
    "        try:\n",
    "            if \"item\" in url.split(\"prlib.ru/\")[1]:   #case for https://www.prlib.ru/item/*** \n",
    "                book = book_json['diva']['1']['options']\n",
    "            elif \"node\" in url.split(\"prlib.ru/\")[1]:     #case for https://www.prlib.ru/node/***\n",
    "                book = book_json['diva']['settings']\n",
    "        except:\n",
    "            log.exception(\"Error, NOTHING FOUND!\")\n",
    "            \n",
    "\n",
    "json_text = bro.get_text(book['objectData'])\n",
    "book_data = json.loads(json_text)\n",
    "pages = book_data['pgs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "056d8273-cd0d-4284-a4f3-a77411b7731d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m page\u001b[38;5;241m=\u001b[39mpages[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m img_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m&JTL=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m      3\u001b[0m             book[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimageDir\u001b[39m\u001b[38;5;124m'\u001b[39m], page[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m], page[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m#поменял здесь немного вид урл, так как по частям качаю\u001b[39;00m\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# брал урл отсюда: https://iipimage.sourceforge.io/documentation/protocol\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pages' is not defined"
     ]
    }
   ],
   "source": [
    "page=pages[0]\n",
    "img_url = 'https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF={}/{}&JTL={},'.format(\n",
    "            book['imageDir'], page['f'], page['m']) #поменял здесь немного вид урл, так как по частям качаю\n",
    "        # брал урл отсюда: https://iipimage.sourceforge.io/documentation/protocol\n",
    "img_url+=\"{}\"\n",
    "\n",
    "headers_pr1.update({'Referer': url})\n",
    "width, height=number_of_images(page[\"d\"][len(page['d']) - 1]['w'],page[\"d\"][len(page['d']) - 1]['h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d41757ac-1c5e-4187-9ca5-e75c7e3d0feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9665071964263916\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start=time.time()\n",
    "nest_asyncio.apply()\n",
    "try:\n",
    "    asyncio.run(async_images_download(img_url,width*height,headers_pr1))\n",
    "except Exception as Argument:\n",
    "    import logging\n",
    "    logging.exception(Argument)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f6c2a-cebc-40c5-86e4-2965545932a3",
   "metadata": {},
   "source": [
    "# GOAL - RUN the DOWNLOADS for ALL IMAGES in a Book at the same time? (try for 3 images)\n",
    "on fail, do it again but to only the needed ones (possibly it'll save a LOT of TIME)\n",
    "200 pages book->50 minutes (now MUCH faster)\n",
    "-all should go one at a time, if Error, Repeat but only with the LEFT ONES! (Use Sephamore to reduce the LAG)\n",
    "-> ALL Time-Consuming things done one after another (LESS WAIT TIME!) by MAGNITUDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc199bf0-bf61-4b93-9708-88be68d5cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,10):\n",
    "    os.makedirs(\"check\\\\HAHA\"+str(i)+\"\\\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9543578-e9cd-4c7a-ab2d-8a1bc1444ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4917900562286377\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import traceback\n",
    "import time\n",
    "start=time.time()\n",
    "async def main():\n",
    "    \n",
    "    tasks=[async_images_download(url,range(20),headers_pr1,\"HAHA\"+str(idx)) for idx, url in enumerate(urls[:10])]\n",
    "    try:\n",
    "        await asyncio.gather(*tasks)\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        \n",
    "nest_asyncio.apply()\n",
    "asyncio.run(main())\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5bb6d0-70db-4bd5-8f7c-ca877eb0f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "global STOP_break\n",
    "STOP_break=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e173d5-091f-458d-8d53-8778a8e6aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import Postprocess\n",
    "async def fetch_image_download(url: str,headers_pr1, i, session,images_folder):\n",
    "    \"\"\" Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)\n",
    "    по url скачиваю картинку и добавляю в file\n",
    "    \"\"\"\n",
    "    #proxies:https://github.com/hamzarana07/multiProxies/tree/main\n",
    "\n",
    "    async with session.get(url, headers=headers_pr1) as response:\n",
    "        with open(os.path.join(images_folder,str(i)+\".jpg\"),\"wb\") as file:\n",
    "            file.write(await response.read())\n",
    "                \n",
    "                \n",
    "async def async_images_download(session,sem1,url,nums,headers_pr1,title,images_folder,image_path,width,height):\n",
    "    \"\"\"Не добавляйте в util.py, у меня тогда asyncio не работал (может баг на моей стороне)\n",
    "    call every tile image to download in async mode\n",
    "    \"\"\"\n",
    "    #for each IMAGE\n",
    "    #https://stackoverflow.com/questions/63347818/aiohttp-client-exceptions-clientconnectorerror-cannot-connect-to-host-stackover\n",
    "    #queue = asyncio.Queue()\n",
    "     #https://blog.csdn.net/y662225dd/article/details/135273140\n",
    "        \n",
    "    for i in nums: #doing it for Every url of subimages:\n",
    "        flag=True #just keep quering the connections (Until ALL IMAGES ARE PRESENT\n",
    "        while flag:  #while check for a complete download:\n",
    "            global STOP_break\n",
    "            if STOP_break:\n",
    "                return\n",
    "            headers_pr1.update({'User-Agent': generate_user_agent(os='win',device_type ='desktop',navigator='chrome') })\n",
    "            try: #catching error here\n",
    "                async with sem1: \n",
    "                    await fetch_image_download(url.format(i),headers_pr1, i,session,images_folder)\n",
    "            except Exception as Argument:\n",
    "                log.exception(\"Error occurred in ASYNCIO\")\n",
    "                await asyncio.sleep(1)\n",
    "            else:\n",
    "                flag=False\n",
    "    #if it's all done, check for each image quality:\n",
    "    check=False\n",
    "    lst=os.listdir(images_folder)\n",
    "    if len(lst)==width*height:\n",
    "        check=True\n",
    "    #check for each file to be non empty:\n",
    "    for file in lst:\n",
    "        if os.path.getsize(os.path.join(images_folder, file))==0:\n",
    "            os.remove(os.path.join(images_folder, file))\n",
    "            check*=False\n",
    "        #else: check stays True\n",
    "    if check: \n",
    "        #after download of all images create the BIG ONE:\n",
    "        await Postprocess(images_folder,width,height, image_path)\n",
    "\n",
    "\n",
    "async def Main_Download(pages,book, title,url):\n",
    "    \"\"\"\n",
    "    Main function, where each url is created and it's later on passed to function to download each page\n",
    "    \"\"\"\n",
    "    #num_of_pages_down=1 #for the time prediction\n",
    "    #start=datetime.datetime.now()#for the time prediction\n",
    " \n",
    "    # and pass the result for DOWNLOAD\n",
    "#ON ERROR CREATE THE DATA FOR DOWNLOAD::    \n",
    "    data={}\n",
    "\n",
    "    counter=0 #check the pages\n",
    "    while counter<6: # CREATE the DATA for Download + create FOLDER strcuture\n",
    "        idx=counter\n",
    "        page=pages[counter]\n",
    "        #force downloading every page\n",
    "\n",
    "        img_url = 'https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF={}/{}&JTL={},'.format(\n",
    "            book['imageDir'], page['f'], page['m']) #поменял здесь немного вид урл, так как по частям качаю\n",
    "        # брал урл отсюда: https://iipimage.sourceforge.io/documentation/protocol\n",
    "        img_url+=\"{}\"\n",
    "        width, height=number_of_images(page[\"d\"][len(page['d']) - 1]['w'],page[\"d\"][len(page['d']) - 1]['h']) \n",
    "        image_short = '%05d.%s' % (idx+1, \"jpg\")\n",
    "        image_path = os.path.join(BOOK_DIR, title, image_short)\n",
    "        headers_pr1.update({'Referer': url})\n",
    "        #created the DATA\n",
    "        #check for what was ALREADY DOWNLOADED\n",
    "        if os.path.exists(image_path) and os.stat(image_path).st_size > 0:\n",
    "            log.info(f'Пропускаю скачанный файл: {image_path}')\n",
    "            counter+=1\n",
    "            #progress(f'  Прогресс: {idx + 1} из {len(pages)} стр. ')\n",
    "        else: \n",
    "            #mkdirs_for_regular_file(image_path)\n",
    "            \n",
    "            images_folder=os.path.join(BOOK_DIR, title,\"images\"+str(counter))\n",
    "            nums=range(width*height)  \n",
    "            try:\n",
    "                os.makedirs(images_folder)\n",
    "            except FileExistsError:\n",
    "                pass  \n",
    "            good_nums=[] #check what subimages for each image are present\n",
    "            for num in nums:\n",
    "                if not (os.path.isfile(os.path.join(images_folder,str(num)+\".jpg\")) and os.path.getsize(os.path.join(images_folder,str(num)+\".jpg\"))!=0):\n",
    "                    good_nums.append(num)\n",
    "            nums=good_nums   \n",
    "            counter+=1\n",
    "            data[idx]=[img_url,image_path,images_folder,headers_pr1,nums,width, height] \n",
    "    \n",
    "    #DOWNLOAD the LEFT images: \n",
    "    try:\n",
    "        #create SUBGROUP:\n",
    "        sem1 = asyncio.Semaphore(1000)\n",
    "        #https://pawelmhm.github.io/asyncio/python/aiohttp/2016/04/22/asyncio-aiohttp.html\n",
    "        coroutines=[]\n",
    "        async with ClientSession(timeout=ClientTimeout(total=8),trust_env=True) as session:\n",
    "            for key, value in data.items(): #iterateing over ALL IMAGES gathered:\n",
    "                coroutines.append(async_images_download(session,sem1,value[0],value[4],value[3],title,value[2],value[1],value[5],value[6]))\n",
    "            await asyncio.gather(*coroutines)\n",
    "            \n",
    "    except Exception as Argument:  #Error coding\n",
    "        time.sleep(2.)\n",
    "        log.exception(\"Error occurred in ASYNCIO\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40da06e2-7ab9-4ce6-97a9-d8a65323209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.774611473083496\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "def PrLib_MAIN(url):\n",
    "    ext = prlDl_params['ext']\n",
    "    global headers_pr2\n",
    "    global headers_pr1\n",
    "    html_text = requests.get(url, headers=headers_pr2).text\n",
    "    \n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    title = soup.head.title.text.split(\"|\")[0]\n",
    "    title = safe_file_name(title)\n",
    "    #get the number of characters in the current path\n",
    "    num_of_characters=150-len(os.path.abspath(os.getcwd()))\n",
    "    if len(title)>165:\n",
    "        title=title[:num_of_characters] + title[-15:]#to have the volume part in the name\n",
    "    \n",
    "    log.info(f'Каталог для загрузки: {title}')\n",
    "    \n",
    "    for script in soup.find_all('script'): #findAll deprecated\n",
    "        st = str(script)\n",
    "        if 'jQuery.extend' in st:\n",
    "            book_json = json.loads(st[st.find('{\"'): st.find(');')])\n",
    "            \n",
    "            try:\n",
    "                if \"item\" in url.split(\"prlib.ru/\")[1]:   #case for https://www.prlib.ru/item/*** \n",
    "                    book = book_json['diva']['1']['options']\n",
    "                elif \"node\" in url.split(\"prlib.ru/\")[1]:     #case for https://www.prlib.ru/node/***\n",
    "                    book = book_json['diva']['settings']\n",
    "            except:\n",
    "                log.exception(\"Error, NOTHING FOUND!\")\n",
    "                return\n",
    "    json_text = bro.get_text(book['objectData'],headers=headers_pr2)\n",
    "    book_data = json.loads(json_text)\n",
    "    pages = book_data['pgs']\n",
    "    # run asyncio:\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(Main_Download(pages, book, title,url))\n",
    "\n",
    "import time\n",
    "start=time.time()\n",
    "PrLib_MAIN(\"https://www.prlib.ru/item/1624135\")\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f8d22c-e06c-4616-9aef-8003b04d2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_images(width, height):\n",
    "    \"\"\"\n",
    "    получаю кол-во картинок по ширине и длине (возможно можно в одну строчку как-то:)\n",
    "    \"\"\"\n",
    "    num_w=width//256\n",
    "    if width%256!=0:\n",
    "        num_w+=1\n",
    "    num_h=height//256\n",
    "    if height%256!=0:\n",
    "        num_h+=1\n",
    "    return int(num_w),int(num_h)  \n",
    "def CV2_Russian(name):\n",
    "    \"\"\"\n",
    "    Чтение картинки с русским названием в пути в cv2\n",
    "    #https://answers.opencv.org/question/205345/imread-and-russian-language-path-to-img/\n",
    "    \"\"\"\n",
    "    f = open(name, \"rb\")\n",
    "    chunk = f.read()\n",
    "    chunk_arr = np.frombuffer(chunk, dtype=np.uint8)\n",
    "    img = cv2.imdecode(chunk_arr, cv2.IMREAD_COLOR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc49ad-671f-4a5f-b01c-e5d3a2115a4d",
   "metadata": {},
   "source": [
    "### PRevious runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551ffd8-cf8b-4284-8221-5d6633756fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1276d3e4-7aff-4126-a747-8f8bd3a610ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF=/var/data/scans/public/7E028BF3-4CBD-429A-9FBB-48364FC9032B/0/10000066_doc1.tiff&JTL=3,{}\n",
      "https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF=/var/data/scans/public/7E028BF3-4CBD-429A-9FBB-48364FC9032B/0/10000067_doc1.tiff&JTL=3,{}\n",
      "7.455045461654663\n"
     ]
    }
   ],
   "source": [
    "STOP_break=False\n",
    "import nest_asyncio\n",
    "import shutil\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "prlDl(\"https://www.prlib.ru/item/710798\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ba6443-eaf3-4a0b-abb3-9aeef0152459",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.prlib.ru/item/825194\"\n",
    "\"\"\"\n",
    "Президентская библиотека имени Б.Н. Ельцина\n",
    "Формат - серия изображений\n",
    "Пример урла книги (HTML) - https://www.prlib.ru/item/420931\n",
    "\"\"\"\n",
    "ext = prlDl_params['ext']\n",
    "global headers_pr1\n",
    "headers_pr2=headers_pr1 \n",
    "headers_pr2.update({\"Host\":\"www.prlib.ru\",\"Origin\": \"https://www.prlib.ru\"})\n",
    "html_text = requests.get(url, headers=headers_pr2).text\n",
    "\n",
    "soup = BeautifulSoup(html_text, 'html.parser')\n",
    "title = soup.head.title.text.split(\"|\")[0]\n",
    "title = safe_file_name(title)\n",
    "#get the number of characters in the current path\n",
    "num_of_characters=150-len(os.path.abspath(os.getcwd()))\n",
    "if len(title)>165:\n",
    "    title=title[:num_of_characters] + title[-15:]#to have the volume part in the name\n",
    "\n",
    "log.info(f'Каталог для загрузки: {title}')\n",
    "\n",
    "for script in soup.find_all('script'): #findAll deprecated\n",
    "    st = str(script)\n",
    "    if 'jQuery.extend' in st:\n",
    "        book_json = json.loads(st[st.find('{\"'): st.find(');')])\n",
    "        \n",
    "        try:\n",
    "            if \"item\" in url.split(\"prlib.ru/\")[1]:   #case for https://www.prlib.ru/item/*** \n",
    "                book = book_json['diva']['1']['options']\n",
    "            elif \"node\" in url.split(\"prlib.ru/\")[1]:     #case for https://www.prlib.ru/node/***\n",
    "                book = book_json['diva']['settings']\n",
    "        except:\n",
    "            log.exception(\"Error, NOTHING FOUND!\")\n",
    "           \n",
    "json_text = bro.get_text(book['objectData'])\n",
    "\n",
    "book_data = json.loads(json_text)\n",
    "pages = book_data['pgs']\n",
    "num_of_pages_down=1 #for the time prediction\n",
    "start=datetime.datetime.now()#for the time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed343c0c-724e-45ab-aaef-dc6b76fde922",
   "metadata": {},
   "outputs": [],
   "source": [
    "global STOP_break\n",
    "counter=0 #check the pages\n",
    "while counter<2:\n",
    "    idx=counter\n",
    "    page=pages[counter]\n",
    "    #force downloading every page, if error REPEAT\n",
    "    if STOP_break:\n",
    "        break\n",
    "    img_url = 'https://content.prlib.ru/fcgi-bin/iipsrv.fcgi?FIF={}/{}&JTL={},'.format(\n",
    "        book['imageDir'], page['f'], page['m']) #поменял здесь немного вид урл, так как по частям качаю\n",
    "    # брал урл отсюда: https://iipimage.sourceforge.io/documentation/protocol\n",
    "    img_url+=\"{}\"\n",
    "    width, height=number_of_images(page[\"d\"][len(page['d']) - 1]['w'],page[\"d\"][len(page['d']) - 1]['h'])\n",
    "    \n",
    "    image_short = '%05d.%s' % (idx+1, ext)\n",
    "    image_path = os.path.join(BOOK_DIR, title, image_short)\n",
    "    print(img_url)\n",
    "    # заменяю все фичи ручками (например тут skip_if_exists), которые были ранее доступны через функции \n",
    "    #(т.к. метод у меня скачивания немного другой)\n",
    "    if os.path.exists(image_path) and os.stat(image_path).st_size > 0:\n",
    "        log.info(f'Пропускаю скачанный файл: {image_path}')\n",
    "        counter+=1\n",
    "       \n",
    "        #progress(f'  Прогресс: {idx + 1} из {len(pages)} стр. ')\n",
    "    else: \n",
    "      \n",
    "        #mkdirs_for_regular_file(image_path)\n",
    "        \n",
    "        images_folder=os.path.join(BOOK_DIR, title,\"images\")\n",
    "        nums=range(width*height)\n",
    "        try:\n",
    "            os.makedirs(images_folder)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        nest_asyncio.apply() # нужен только чтобы async работал нормально в Jupyter ( https://pypi.org/project/nest-asyncio/)\n",
    "        # получить все данные с картиники:\n",
    "\n",
    "        headers_pr1.update({'Referer': url})\n",
    "        \n",
    "        flag=True #для проверки на хороший requests\n",
    "        #check for all images already downloaded?\n",
    "        \n",
    "        while flag: #just keep quering the connection\n",
    "            #check what files were already downloaded:\n",
    "            good_nums=[]\n",
    "            for num in nums:\n",
    "                if not (os.path.isfile(os.path.join(images_folder,str(num)+\".jpg\")) and os.path.getsize(os.path.join(images_folder,str(num)+\".jpg\"))!=0):\n",
    "                    good_nums.append(num)\n",
    "            nums=good_nums\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                asyncio.run(async_images_download(img_url,nums,headers_pr1,title)) #Downgrade to 3.6.2  #Using Python 3.8 https://blog.csdn.net/y662225dd/article/details/135273140\n",
    "                #loop = asyncio.get_event_loop() #for old version of aiohttp: 3.6.2\n",
    "                #loop.run_until_complete(async_images(img_url,width*height,headers))\n",
    "            except Exception as Argument:  #Error coding\n",
    "                time.sleep(2.)\n",
    "\n",
    "                log.exception(\"Error occurred in ASYNCIO\") \n",
    "            else:\n",
    "                check=False\n",
    "                time.sleep(1.0)\n",
    "                lst=os.listdir(images_folder)\n",
    "                if len(lst)==width*height:\n",
    "                    check=True\n",
    "                #check for each file to be non empty:\n",
    "                for file in lst:\n",
    "                    if os.path.getsize(os.path.join(images_folder, file))==0:\n",
    "                        os.remove(os.path.join(images_folder, file))\n",
    "                        check*=False\n",
    "                    #else: check stays True\n",
    "                if check: #stop repeating                        \n",
    "                    flag=False\n",
    "        \n",
    "        # просессить все данные и в конце вывести картинку\n",
    "        if Postprocess(images_folder,width,height, image_path):\n",
    "          counter+=1\n",
    "          num_of_pages_down+=1\n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87e89c2c-ae8b-4bf9-aa44-4842b33372d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.prlib.ru/item/443983\"\n",
    "html_text = requests.get(url, headers=headers_pr2).text\n",
    "\n",
    "soup = BeautifulSoup(html_text, 'html.parser')\n",
    "title = soup.head.title.text.split(\"|\")[0]\n",
    "soup.find_all(id=\"diva-1-num-pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fd323cd-1655-4890-ad72-96609fcb2b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for script in soup.find_all('script'): #findAll deprecated\n",
    "    st = str(script)\n",
    "    if 'jQuery.extend' in st:\n",
    "        book_json = json.loads(st[st.find('{\"'): st.find(');')])\n",
    "        \n",
    "        try:\n",
    "            if \"item\" in url.split(\"prlib.ru/\")[1]:   #case for https://www.prlib.ru/item/*** \n",
    "                book = book_json['diva']['1']['options']\n",
    "            elif \"node\" in url.split(\"prlib.ru/\")[1]:     #case for https://www.prlib.ru/node/***\n",
    "                book = book_json['diva']['settings']\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "json_text = requests.get(book['objectData'],headers=headers_pr2).text\n",
    "book_data = json.loads(json_text)\n",
    "pages = book_data['pgs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fdf16fef-763c-49ed-9f4a-3ebb86098018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25ede1-9187-4515-8a5f-3fc0e02c5e9d",
   "metadata": {},
   "source": [
    "## Archive.org metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ef4ae52-682d-4067-ad8b-b4c6bd2498d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from internetarchive import get_session\n",
    "c = {'s3': {'access': 'qJaX9KKXhXkzoN5o', 'secret': 'mmI4XUkxM9O8gZ15'}}\n",
    "s = get_session(config=c)\n",
    "#GET ALL THE URLS\n",
    "query='uploader:\"pavelserebrjanyi@gmail.com\" AND mediatype:texts'\n",
    "items=s.search_items(query, fields=[\"source_url\"])\n",
    "source_urls=[]\n",
    "for item in items:\n",
    "    source_urls.append(item[\"source_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04cc738c-fa3f-496f-a572-b3f3509f4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://archive.org/details/@qwert290?query=source_url%3A%22https%3A%2F%2Fwww.prlib.ru%2Fitem%2F{}%22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bafa58c6-2600-4866-836f-2b6b77230862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.prlib.ru/item/442175', 'https://www.prlib.ru/item/450992', 'https://www.prlib.ru/item/399473']\n"
     ]
    }
   ],
   "source": [
    "s=set()\n",
    "dup=[]\n",
    "for n in source_urls:\n",
    "    if n in s:\n",
    "        dup.append(n)\n",
    "    else:\n",
    "        s.add(n)\n",
    "print(dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e94b393-9eec-4afe-93f9-f7775687aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://archive.org/details/@qwert290?query=source_url%3A%22https%3A%2F%2Fwww.prlib.ru%2Fitem%2F{}%22\"\n",
    "urls=[url.format(d.split(\"/\")[-1]) for d in dup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "400491c2-29ad-4fef-aa9c-d488bee51558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://archive.org/details/@qwert290?query=source_url%3A%22https%3A%2F%2Fwww.prlib.ru%2Fitem%2F429133%22',\n",
       " 'https://archive.org/details/@qwert290?query=source_url%3A%22https%3A%2F%2Fwww.prlib.ru%2Fitem%2F442175%22',\n",
       " 'https://archive.org/details/@qwert290?query=source_url%3A%22https%3A%2F%2Fwww.prlib.ru%2Fitem%2F450992%22',\n",
       " 'https://archive.org/details/@qwert290?query=source_url%3A%22https%3A%2F%2Fwww.prlib.ru%2Fitem%2F426629%22',\n",
       " 'https://archive.org/details/@qwert290?query=source_url%3A%22https%3A%2F%2Fwww.prlib.ru%2Fitem%2F356203%22',\n",
       " 'https://archive.org/details/@qwert290?query=source_url%3A%22https%3A%2F%2Fwww.prlib.ru%2Fitem%2F693241%22']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8de48fd-555c-44e4-ada8-d5dc2509c09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Booklead\\\\test\\\\booklead\\\\NEW\\\\booklead_2.0\\\\books\\\\testing_testings123.zip'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil, os\n",
    "title=\"Брак и семья\"\n",
    "root=\"books\\\\\"+title\n",
    "new_title=\"testing_testings123\"\n",
    "root=\"books\\\\\"+new_title\n",
    "os.rename(\"books\\\\\"+title, \"books\\\\\"+new_title)\n",
    "for dir, subdirs, files in os.walk(root):\n",
    "    for f in files:\n",
    "        f_new = new_title+ f\n",
    "        os.rename(os.path.join(root, f), os.path.join(root, f_new))\n",
    "\n",
    "\n",
    "shutil.make_archive(root,\n",
    "                    'zip',\n",
    "                    root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd62e588-6468-43d7-84f2-f53e34832499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import internetarchive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0b456a3-5768-4dcf-9e4d-8e8ab66691b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " uploading testing_testings123_images.zip: 100%|██████████| 30/30 [00:40<00:00,  1.34s/MiB]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Response [200]>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internetarchive.upload(new_title, new_name, metadata, verify=True,validate_identifier=True,verbose=True,retries=20, retries_sleep =3, queue_derive=True,access_key=\"qJaX9KKXhXkzoN5o\", secret_key=\"mmI4XUkxM9O8gZ15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b713848d-f498-4197-935a-2e8b9b491a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=fetch_metadata(\"https://www.prlib.ru/item/399473\", headers_pr2)\n",
    "\n",
    "\n",
    "def fetch_metadata(url,headers_pr2):\n",
    "        #fetch metadata:\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "\n",
    "    html_text =requests.get(url,headers=headers_pr2).text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    if url.split(\"https://\")[1][4:9]==\"prlib\": #prlib.ru metadata\n",
    "        title = soup.head.title.text.split(\"|\")[0]\n",
    "        full_title=title\n",
    "        if len(title)>210:\n",
    "            end_on_space=\" \".join(title[:210].split(\" \")[:-1])\n",
    "            title=end_on_space+\" ...\"\n",
    "        authors=soup.find(\"ul\",{\"class\":\"field field-name-field-book-author field-type-taxonomy-term-reference field-label-hidden\"})\n",
    "        author_=[]\n",
    "        try:\n",
    "            for author in authors.find_all(\"a\"):\n",
    "                author_.append(author.text)\n",
    "        except:\n",
    "            author_=\"\"\n",
    "            \n",
    "        description=\"\"\n",
    "        td_check=soup.find(\"div\",{\"class\":\"field field-name-field-book-bd field-type-text-long field-label-hidden\"}).find_all(\"td\")\n",
    "        if len(td_check)!=0: #check for zero td\n",
    "            for desc in td_check[1:][:-1]:\n",
    "                if desc.td is None and desc.get_text(strip=True)!=\"\":\n",
    "                    description+=desc.get_text(strip=True).replace(\"\\n\",\"\")+\"\\n\"\n",
    "        #collection+catalog form a subject:\n",
    "        #Catalogs\n",
    "        subjects=[]\n",
    "        catalogs=soup.find_all(class_=\"df-bbk\")\n",
    "        if len(catalogs)!=0:\n",
    "            for subject in catalogs[0].find_all(\"li\"):\n",
    "                subjects.append(subject.text.strip())\n",
    "        \n",
    "        #Collections: #ADD THEM to DESCRIPTION\n",
    "        subject_set=[]\n",
    "        collections=soup.find_all(class_=\"df-relations\")\n",
    "        if len(collections)!=0:\n",
    "            for subject in collections[0].find_all(\"li\"):\n",
    "                description+=subject.text+\"\\n\"\n",
    "                for element in subject.text.split(\" → \")[:-2]:\n",
    "                    subject_set.append(element)\n",
    "        subjects=subjects+list(dict.fromkeys(subject_set))\n",
    "        #language detection:\n",
    "        lang=detect(title)\n",
    "        dict_lang={\"de\":\"German\", \"en\":\"English\"}\n",
    "        if lang in list(dict_lang.keys()):\n",
    "            language=dict_lang[lang]\n",
    "        else:\n",
    "            language=\"Russian\"\n",
    "        \n",
    "        #add \"date\" after the data is scrapped\n",
    "        #search by date in .csv?\n",
    "        \n",
    "        dataset=\"PrLib_Dataset.csv\"\n",
    "        #adding date\n",
    "        date=\"\"\n",
    "        if os.path.exists(dataset):\n",
    "            with open(dataset, mode ='r',encoding=\"utf-8\")as file:\n",
    "              csvFile = csv.reader(file)\n",
    "              for lines in csvFile:\n",
    "                    if url==lines[2]:\n",
    "                        date=lines[0]\n",
    "    return {\n",
    "        \"collection\":[\"russian-online-libraries\"],\n",
    "        \"creator\" : author_,\n",
    "        \"language\" : language,\n",
    "        \"mediatype\" : \"texts\",\n",
    "        \"title\" : title,\n",
    "        \"Full_title\": full_title,\n",
    "        \"description\":   description,\n",
    "        \"subject\":subjects,\n",
    "        \"date\":date,\n",
    "        \"Source_url\": url\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ebfa807-330a-4c34-8266-c5f18bb45832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import asyncio\n",
    "from aiohttp import ClientSession,TCPConnector, ClientTimeout\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "#import nest_asyncio #used for debugging\n",
    "from util import CV2_Russian, number_of_images, Postprocess, Time_Processing,archive_ia, fetch_metadata, CheckArchiveForWrites\n",
    "import cv2\n",
    "import random\n",
    "import img2pdf\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from util import get_logger\n",
    "from util import md5_hex, to_float, cut_bom, perror, progress, ptext, safe_file_name, Browser, select_one_text_optional\n",
    "from util import select_one_text_required, select_one_attr_required, gwar_fix_json,mkdirs_for_regular_file\n",
    "from util import user_agents\n",
    "from user_agent import generate_user_agent\n",
    "import logging\n",
    "import threading\n",
    "import requests\n",
    "from internetarchive import search_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52482af4-4f94-45e8-8da1-bd49db682015",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_pr1 = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br, zstd\", \n",
    "    \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8,ru;q=0.7\",\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "    'If-Modified-Since': 'Tue, 20 Dec 2016 02:17:59 GMT',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Sec-Gpc':'1',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',\n",
    "    'dnt': '1',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"137\", \"Google Chrome\";v=\"137\", \"Not/A)Brand\";v=\"24\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-gpc': '1',\n",
    "    'Host':'content.prlib.ru',\n",
    "    'Origin':'https://content.prlib.ru'\n",
    "}\n",
    "headers_pr2=headers_pr1 \n",
    "headers_pr2.update({\"Host\":\"www.prlib.ru\",\"Origin\": \"https://www.prlib.ru\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
